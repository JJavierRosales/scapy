{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import standard libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Parent working directory: /Users/jjrr/Documents/SCA-Project/Tool\n"]}],"source":["import torch\n","import torch.nn as nn  # we'll use this a lot going forward!\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import warnings\n","\n","# Import matplotlib library and setup environment for plots\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","from matplotlib import pyplot as plt, rc\n","\n","# Import json library and create function to format dictionaries.\n","import json\n","format_json = lambda x: json.dumps(x, indent=4)\n","\n","# Import pandas and set pandas DataFrame visualization parameters\n","from IPython.display import display\n","import pandas as pd\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None\n","\n","# Set rendering parameters to use TeX font if not working on Juno app.\n","from pathlib import Path\n","import os\n","if not '/private/var/' in os.getcwd():\n","    rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 11})\n","    rc('text', usetex=True)\n","    \n","# Get current working directory path for the tool parent folder and print it.\n","parent_folder = 'Tool'\n","cwd = str(Path(os.getcwd()[:os.getcwd().index(parent_folder)+len(parent_folder)]))\n","print('Parent working directory: %s' % cwd)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import user defined libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":false},"outputs":[],"source":["# Import custom libraries from local folder.\n","import sys\n","sys.path.append(\"..\")\n","\n","from library.irplib import utils, eda, config, sdg\n","from library.irplib import rnn"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data preparation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import training dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["Events suitable for training (More than 5 CDMs): 9400 ( 71.5%)\n","Time sequences with event_id integrity per feature: 94699\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>event_id</th>\n","      <th>time_to_tca</th>\n","      <th>mission_id</th>\n","      <th>risk</th>\n","      <th>max_risk_estimate</th>\n","      <th>max_risk_scaling</th>\n","      <th>miss_distance</th>\n","      <th>relative_speed</th>\n","      <th>relative_position_r</th>\n","      <th>relative_position_t</th>\n","      <th>relative_position_n</th>\n","      <th>relative_velocity_r</th>\n","      <th>relative_velocity_t</th>\n","      <th>relative_velocity_n</th>\n","      <th>t_time_lastob_start</th>\n","      <th>t_time_lastob_end</th>\n","      <th>t_recommended_od_span</th>\n","      <th>t_actual_od_span</th>\n","      <th>t_obs_available</th>\n","      <th>t_obs_used</th>\n","      <th>t_residuals_accepted</th>\n","      <th>t_weighted_rms</th>\n","      <th>t_rcs_estimate</th>\n","      <th>t_cd_area_over_mass</th>\n","      <th>t_cr_area_over_mass</th>\n","      <th>t_sedr</th>\n","      <th>t_j2k_sma</th>\n","      <th>t_j2k_ecc</th>\n","      <th>t_j2k_inc</th>\n","      <th>t_ct_r</th>\n","      <th>t_cn_r</th>\n","      <th>t_cn_t</th>\n","      <th>t_crdot_r</th>\n","      <th>t_crdot_t</th>\n","      <th>t_crdot_n</th>\n","      <th>t_ctdot_r</th>\n","      <th>t_ctdot_t</th>\n","      <th>t_ctdot_n</th>\n","      <th>t_ctdot_rdot</th>\n","      <th>t_cndot_r</th>\n","      <th>t_cndot_t</th>\n","      <th>t_cndot_n</th>\n","      <th>t_cndot_rdot</th>\n","      <th>t_cndot_tdot</th>\n","      <th>c_object_type</th>\n","      <th>c_time_lastob_start</th>\n","      <th>c_time_lastob_end</th>\n","      <th>c_recommended_od_span</th>\n","      <th>c_actual_od_span</th>\n","      <th>c_obs_available</th>\n","      <th>c_obs_used</th>\n","      <th>c_residuals_accepted</th>\n","      <th>c_weighted_rms</th>\n","      <th>c_rcs_estimate</th>\n","      <th>c_cd_area_over_mass</th>\n","      <th>c_cr_area_over_mass</th>\n","      <th>c_sedr</th>\n","      <th>c_j2k_sma</th>\n","      <th>c_j2k_ecc</th>\n","      <th>c_j2k_inc</th>\n","      <th>c_ct_r</th>\n","      <th>c_cn_r</th>\n","      <th>c_cn_t</th>\n","      <th>c_crdot_r</th>\n","      <th>c_crdot_t</th>\n","      <th>c_crdot_n</th>\n","      <th>c_ctdot_r</th>\n","      <th>c_ctdot_t</th>\n","      <th>c_ctdot_n</th>\n","      <th>c_ctdot_rdot</th>\n","      <th>c_cndot_r</th>\n","      <th>c_cndot_t</th>\n","      <th>c_cndot_n</th>\n","      <th>c_cndot_rdot</th>\n","      <th>c_cndot_tdot</th>\n","      <th>t_span</th>\n","      <th>c_span</th>\n","      <th>t_h_apo</th>\n","      <th>t_h_per</th>\n","      <th>c_h_apo</th>\n","      <th>c_h_per</th>\n","      <th>geocentric_latitude</th>\n","      <th>azimuth</th>\n","      <th>elevation</th>\n","      <th>mahalanobis_distance</th>\n","      <th>t_position_covariance_det</th>\n","      <th>c_position_covariance_det</th>\n","      <th>t_sigma_r</th>\n","      <th>c_sigma_r</th>\n","      <th>t_sigma_t</th>\n","      <th>c_sigma_t</th>\n","      <th>t_sigma_n</th>\n","      <th>c_sigma_n</th>\n","      <th>t_sigma_rdot</th>\n","      <th>c_sigma_rdot</th>\n","      <th>t_sigma_tdot</th>\n","      <th>c_sigma_tdot</th>\n","      <th>t_sigma_ndot</th>\n","      <th>c_sigma_ndot</th>\n","      <th>F10</th>\n","      <th>F3M</th>\n","      <th>SSN</th>\n","      <th>AP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>2</td>\n","      <td>0.997639</td>\n","      <td>2</td>\n","      <td>-10.816161</td>\n","      <td>-6.601713</td>\n","      <td>13.293159</td>\n","      <td>0.381700</td>\n","      <td>0.71740</td>\n","      <td>-0.289400</td>\n","      <td>-0.070069</td>\n","      <td>0.244292</td>\n","      <td>0.022571</td>\n","      <td>-0.68960</td>\n","      <td>-0.197855</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.92</td>\n","      <td>3.92</td>\n","      <td>444</td>\n","      <td>442</td>\n","      <td>99.4</td>\n","      <td>1.094</td>\n","      <td>3.4505</td>\n","      <td>0.638000</td>\n","      <td>0.156500</td>\n","      <td>0.000019</td>\n","      <td>1.208407</td>\n","      <td>0.000860</td>\n","      <td>0.564323</td>\n","      <td>-0.099768</td>\n","      <td>0.357995</td>\n","      <td>-0.122174</td>\n","      <td>0.085472</td>\n","      <td>-0.999674</td>\n","      <td>0.121504</td>\n","      <td>-0.999114</td>\n","      <td>0.057809</td>\n","      <td>-0.353866</td>\n","      <td>-0.043471</td>\n","      <td>-0.025138</td>\n","      <td>0.087954</td>\n","      <td>-0.430583</td>\n","      <td>-0.088821</td>\n","      <td>0.021409</td>\n","      <td>UNKNOWN</td>\n","      <td>180.0</td>\n","      <td>2.0</td>\n","      <td>13.87</td>\n","      <td>13.87</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.838</td>\n","      <td>NaN</td>\n","      <td>0.040715</td>\n","      <td>0.112876</td>\n","      <td>0.000726</td>\n","      <td>0.087532</td>\n","      <td>0.001367</td>\n","      <td>0.847990</td>\n","      <td>-0.068526</td>\n","      <td>0.636970</td>\n","      <td>-0.038214</td>\n","      <td>0.064305</td>\n","      <td>-0.999989</td>\n","      <td>0.036762</td>\n","      <td>-0.996314</td>\n","      <td>0.153806</td>\n","      <td>-0.634961</td>\n","      <td>-0.149627</td>\n","      <td>0.715984</td>\n","      <td>-0.159057</td>\n","      <td>0.953945</td>\n","      <td>0.156803</td>\n","      <td>-0.723349</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974572</td>\n","      <td>1.002050</td>\n","      <td>0.024863</td>\n","      <td>1.112026</td>\n","      <td>0.319779</td>\n","      <td>-0.080044</td>\n","      <td>-0.003155</td>\n","      <td>0.057604</td>\n","      <td>4.110620e+06</td>\n","      <td>2.715773e+18</td>\n","      <td>0.137597</td>\n","      <td>0.346868</td>\n","      <td>0.312163</td>\n","      <td>0.659368</td>\n","      <td>0.031019</td>\n","      <td>0.336601</td>\n","      <td>-0.117197</td>\n","      <td>0.230077</td>\n","      <td>-0.291892</td>\n","      <td>-0.081841</td>\n","      <td>-0.346900</td>\n","      <td>-0.067535</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>0.955944</td>\n","      <td>2</td>\n","      <td>-10.850473</td>\n","      <td>-6.603452</td>\n","      <td>13.374242</td>\n","      <td>0.382767</td>\n","      <td>0.71740</td>\n","      <td>-0.290275</td>\n","      <td>-0.070336</td>\n","      <td>0.244959</td>\n","      <td>0.022571</td>\n","      <td>-0.68960</td>\n","      <td>-0.197855</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.86</td>\n","      <td>3.86</td>\n","      <td>444</td>\n","      <td>442</td>\n","      <td>99.4</td>\n","      <td>1.099</td>\n","      <td>3.4505</td>\n","      <td>0.604200</td>\n","      <td>0.180200</td>\n","      <td>0.000017</td>\n","      <td>1.208407</td>\n","      <td>0.000861</td>\n","      <td>0.564323</td>\n","      <td>-0.005874</td>\n","      <td>0.360471</td>\n","      <td>-0.036075</td>\n","      <td>-0.002789</td>\n","      <td>-0.999876</td>\n","      <td>0.035870</td>\n","      <td>-0.997255</td>\n","      <td>-0.068114</td>\n","      <td>-0.357012</td>\n","      <td>0.076754</td>\n","      <td>-0.027154</td>\n","      <td>0.084268</td>\n","      <td>-0.442266</td>\n","      <td>-0.085037</td>\n","      <td>0.020991</td>\n","      <td>UNKNOWN</td>\n","      <td>180.0</td>\n","      <td>2.0</td>\n","      <td>13.87</td>\n","      <td>13.87</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.838</td>\n","      <td>NaN</td>\n","      <td>0.040715</td>\n","      <td>0.112876</td>\n","      <td>0.000726</td>\n","      <td>0.087532</td>\n","      <td>0.001367</td>\n","      <td>0.847990</td>\n","      <td>-0.067750</td>\n","      <td>0.636974</td>\n","      <td>-0.038143</td>\n","      <td>0.063521</td>\n","      <td>-0.999989</td>\n","      <td>0.036689</td>\n","      <td>-0.996313</td>\n","      <td>0.153053</td>\n","      <td>-0.634998</td>\n","      <td>-0.148865</td>\n","      <td>0.715914</td>\n","      <td>-0.158753</td>\n","      <td>0.953971</td>\n","      <td>0.156495</td>\n","      <td>-0.723302</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974579</td>\n","      <td>1.002044</td>\n","      <td>0.024863</td>\n","      <td>1.112028</td>\n","      <td>0.319783</td>\n","      <td>-0.080044</td>\n","      <td>-0.003155</td>\n","      <td>0.050715</td>\n","      <td>1.158624e+07</td>\n","      <td>2.704770e+18</td>\n","      <td>0.137291</td>\n","      <td>0.346862</td>\n","      <td>0.343134</td>\n","      <td>0.659245</td>\n","      <td>0.032259</td>\n","      <td>0.336599</td>\n","      <td>-0.086135</td>\n","      <td>0.229954</td>\n","      <td>-0.291868</td>\n","      <td>-0.081850</td>\n","      <td>-0.346051</td>\n","      <td>-0.067537</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.895711</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.217958</td>\n","      <td>426.808532</td>\n","      <td>0.313083</td>\n","      <td>0.71735</td>\n","      <td>-0.174700</td>\n","      <td>-0.057516</td>\n","      <td>0.200498</td>\n","      <td>0.020571</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.85</td>\n","      <td>3.85</td>\n","      <td>447</td>\n","      <td>445</td>\n","      <td>99.4</td>\n","      <td>1.113</td>\n","      <td>3.4505</td>\n","      <td>0.575950</td>\n","      <td>0.163283</td>\n","      <td>0.000016</td>\n","      <td>1.208441</td>\n","      <td>0.000862</td>\n","      <td>0.564323</td>\n","      <td>-0.222621</td>\n","      <td>0.425875</td>\n","      <td>-0.149746</td>\n","      <td>0.206756</td>\n","      <td>-0.999517</td>\n","      <td>0.147289</td>\n","      <td>-0.999479</td>\n","      <td>0.191052</td>\n","      <td>-0.423717</td>\n","      <td>-0.175085</td>\n","      <td>0.082662</td>\n","      <td>0.017007</td>\n","      <td>-0.405439</td>\n","      <td>-0.018617</td>\n","      <td>-0.083820</td>\n","      <td>UNKNOWN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>0.025977</td>\n","      <td>0.563595</td>\n","      <td>0.065183</td>\n","      <td>-0.045196</td>\n","      <td>-0.999602</td>\n","      <td>-0.075887</td>\n","      <td>-0.999774</td>\n","      <td>-0.006036</td>\n","      <td>-0.564147</td>\n","      <td>0.025308</td>\n","      <td>0.703561</td>\n","      <td>-0.027022</td>\n","      <td>0.916588</td>\n","      <td>0.007301</td>\n","      <td>-0.706289</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974617</td>\n","      <td>1.002059</td>\n","      <td>0.024838</td>\n","      <td>1.112860</td>\n","      <td>0.319517</td>\n","      <td>-0.080050</td>\n","      <td>-0.002875</td>\n","      <td>0.088636</td>\n","      <td>3.780413e+06</td>\n","      <td>7.644201e+13</td>\n","      <td>0.143460</td>\n","      <td>0.242464</td>\n","      <td>0.300218</td>\n","      <td>0.489541</td>\n","      <td>0.037578</td>\n","      <td>0.279063</td>\n","      <td>-0.129056</td>\n","      <td>0.060121</td>\n","      <td>-0.286243</td>\n","      <td>-0.186705</td>\n","      <td>-0.345671</td>\n","      <td>-0.102776</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2</td>\n","      <td>0.863193</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.271078</td>\n","      <td>181.496778</td>\n","      <td>0.314033</td>\n","      <td>0.71735</td>\n","      <td>-0.175000</td>\n","      <td>-0.057690</td>\n","      <td>0.201104</td>\n","      <td>0.020571</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.83</td>\n","      <td>3.83</td>\n","      <td>451</td>\n","      <td>449</td>\n","      <td>99.4</td>\n","      <td>1.122</td>\n","      <td>3.4479</td>\n","      <td>0.558300</td>\n","      <td>0.168567</td>\n","      <td>0.000016</td>\n","      <td>1.208441</td>\n","      <td>0.000862</td>\n","      <td>0.564323</td>\n","      <td>-0.230120</td>\n","      <td>0.236754</td>\n","      <td>-0.045980</td>\n","      <td>0.222933</td>\n","      <td>-0.999848</td>\n","      <td>0.047067</td>\n","      <td>-0.998058</td>\n","      <td>0.169099</td>\n","      <td>-0.236669</td>\n","      <td>-0.161825</td>\n","      <td>0.104967</td>\n","      <td>0.002330</td>\n","      <td>-0.433469</td>\n","      <td>-0.003761</td>\n","      <td>-0.106536</td>\n","      <td>UNKNOWN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>-0.199922</td>\n","      <td>0.552272</td>\n","      <td>0.010836</td>\n","      <td>0.192984</td>\n","      <td>-0.999944</td>\n","      <td>-0.014799</td>\n","      <td>-0.999656</td>\n","      <td>0.192663</td>\n","      <td>-0.554558</td>\n","      <td>-0.185664</td>\n","      <td>0.694842</td>\n","      <td>-0.051859</td>\n","      <td>0.916218</td>\n","      <td>0.044474</td>\n","      <td>-0.699265</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974618</td>\n","      <td>1.002058</td>\n","      <td>0.024838</td>\n","      <td>1.112860</td>\n","      <td>0.319521</td>\n","      <td>-0.080050</td>\n","      <td>-0.002875</td>\n","      <td>0.067247</td>\n","      <td>8.405442e+06</td>\n","      <td>5.496282e+14</td>\n","      <td>0.138350</td>\n","      <td>0.244102</td>\n","      <td>0.331685</td>\n","      <td>0.550805</td>\n","      <td>0.031732</td>\n","      <td>0.279068</td>\n","      <td>-0.097650</td>\n","      <td>0.121492</td>\n","      <td>-0.291678</td>\n","      <td>-0.185292</td>\n","      <td>-0.346361</td>\n","      <td>-0.102712</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2</td>\n","      <td>0.815959</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.277448</td>\n","      <td>187.525360</td>\n","      <td>0.316917</td>\n","      <td>0.71735</td>\n","      <td>-0.177475</td>\n","      <td>-0.058246</td>\n","      <td>0.202942</td>\n","      <td>0.020714</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.72</td>\n","      <td>3.72</td>\n","      <td>466</td>\n","      <td>464</td>\n","      <td>99.5</td>\n","      <td>1.143</td>\n","      <td>3.4479</td>\n","      <td>0.589075</td>\n","      <td>0.233667</td>\n","      <td>0.000016</td>\n","      <td>1.208439</td>\n","      <td>0.000863</td>\n","      <td>0.564323</td>\n","      <td>0.173348</td>\n","      <td>0.318120</td>\n","      <td>0.169947</td>\n","      <td>-0.187696</td>\n","      <td>-0.999456</td>\n","      <td>-0.170031</td>\n","      <td>-0.999407</td>\n","      <td>-0.207145</td>\n","      <td>-0.321772</td>\n","      <td>0.221392</td>\n","      <td>0.210865</td>\n","      <td>0.006796</td>\n","      <td>-0.381647</td>\n","      <td>-0.010702</td>\n","      <td>-0.209821</td>\n","      <td>UNKNOWN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>-0.194648</td>\n","      <td>0.552833</td>\n","      <td>0.011237</td>\n","      <td>0.187592</td>\n","      <td>-0.999942</td>\n","      <td>-0.015262</td>\n","      <td>-0.999649</td>\n","      <td>0.187206</td>\n","      <td>-0.555115</td>\n","      <td>-0.180088</td>\n","      <td>0.695361</td>\n","      <td>-0.051522</td>\n","      <td>0.916218</td>\n","      <td>0.044023</td>\n","      <td>-0.699780</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974628</td>\n","      <td>1.002045</td>\n","      <td>0.024838</td>\n","      <td>1.112862</td>\n","      <td>0.319533</td>\n","      <td>-0.080050</td>\n","      <td>-0.002895</td>\n","      <td>0.097371</td>\n","      <td>1.933940e+06</td>\n","      <td>5.328544e+14</td>\n","      <td>0.131819</td>\n","      <td>0.244034</td>\n","      <td>0.291445</td>\n","      <td>0.549841</td>\n","      <td>0.033845</td>\n","      <td>0.279063</td>\n","      <td>-0.137807</td>\n","      <td>0.120525</td>\n","      <td>-0.297043</td>\n","      <td>-0.185359</td>\n","      <td>-0.344734</td>\n","      <td>-0.102712</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.768235</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.278272</td>\n","      <td>190.090568</td>\n","      <td>0.318950</td>\n","      <td>0.71735</td>\n","      <td>-0.177575</td>\n","      <td>-0.058596</td>\n","      <td>0.204253</td>\n","      <td>0.020714</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.69</td>\n","      <td>3.69</td>\n","      <td>478</td>\n","      <td>475</td>\n","      <td>99.5</td>\n","      <td>1.119</td>\n","      <td>3.4479</td>\n","      <td>0.590700</td>\n","      <td>0.239400</td>\n","      <td>0.000016</td>\n","      <td>1.208438</td>\n","      <td>0.000862</td>\n","      <td>0.564323</td>\n","      <td>-0.115261</td>\n","      <td>0.255582</td>\n","      <td>0.007605</td>\n","      <td>0.107766</td>\n","      <td>-0.999832</td>\n","      <td>-0.008410</td>\n","      <td>-0.998007</td>\n","      <td>0.052449</td>\n","      <td>-0.257121</td>\n","      <td>-0.044921</td>\n","      <td>0.288501</td>\n","      <td>-0.023265</td>\n","      <td>-0.359223</td>\n","      <td>0.022849</td>\n","      <td>-0.288807</td>\n","      <td>UNKNOWN</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>-0.193936</td>\n","      <td>0.552834</td>\n","      <td>0.011532</td>\n","      <td>0.186793</td>\n","      <td>-0.999941</td>\n","      <td>-0.015606</td>\n","      <td>-0.999640</td>\n","      <td>0.186100</td>\n","      <td>-0.555175</td>\n","      <td>-0.178894</td>\n","      <td>0.695308</td>\n","      <td>-0.051279</td>\n","      <td>0.916219</td>\n","      <td>0.043688</td>\n","      <td>-0.699793</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974624</td>\n","      <td>1.002047</td>\n","      <td>0.024838</td>\n","      <td>1.112863</td>\n","      <td>0.319541</td>\n","      <td>-0.080050</td>\n","      <td>-0.002895</td>\n","      <td>0.074766</td>\n","      <td>5.890298e+06</td>\n","      <td>5.200286e+14</td>\n","      <td>0.132682</td>\n","      <td>0.244030</td>\n","      <td>0.323464</td>\n","      <td>0.549077</td>\n","      <td>0.033580</td>\n","      <td>0.279059</td>\n","      <td>-0.105778</td>\n","      <td>0.119761</td>\n","      <td>-0.296931</td>\n","      <td>-0.185369</td>\n","      <td>-0.349052</td>\n","      <td>-0.102712</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2</td>\n","      <td>0.718416</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.283246</td>\n","      <td>188.270059</td>\n","      <td>0.315300</td>\n","      <td>0.71735</td>\n","      <td>-0.178700</td>\n","      <td>-0.057932</td>\n","      <td>0.201911</td>\n","      <td>0.020714</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.63</td>\n","      <td>3.63</td>\n","      <td>478</td>\n","      <td>475</td>\n","      <td>99.5</td>\n","      <td>1.129</td>\n","      <td>3.4479</td>\n","      <td>0.633800</td>\n","      <td>0.265750</td>\n","      <td>0.000017</td>\n","      <td>1.208439</td>\n","      <td>0.000862</td>\n","      <td>0.564324</td>\n","      <td>-0.188946</td>\n","      <td>0.174655</td>\n","      <td>-0.026595</td>\n","      <td>0.182648</td>\n","      <td>-0.999807</td>\n","      <td>0.027637</td>\n","      <td>-0.998078</td>\n","      <td>0.127830</td>\n","      <td>-0.174536</td>\n","      <td>-0.121472</td>\n","      <td>0.286378</td>\n","      <td>-0.055995</td>\n","      <td>-0.380081</td>\n","      <td>0.054933</td>\n","      <td>-0.286015</td>\n","      <td>UNKNOWN</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>-0.198442</td>\n","      <td>0.552433</td>\n","      <td>0.010928</td>\n","      <td>0.191476</td>\n","      <td>-0.999943</td>\n","      <td>-0.014906</td>\n","      <td>-0.999639</td>\n","      <td>0.190277</td>\n","      <td>-0.554820</td>\n","      <td>-0.183248</td>\n","      <td>0.695002</td>\n","      <td>-0.051780</td>\n","      <td>0.916216</td>\n","      <td>0.044368</td>\n","      <td>-0.699527</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974626</td>\n","      <td>1.002048</td>\n","      <td>0.024838</td>\n","      <td>1.112861</td>\n","      <td>0.319526</td>\n","      <td>-0.080050</td>\n","      <td>-0.002895</td>\n","      <td>0.078619</td>\n","      <td>3.582253e+06</td>\n","      <td>5.455925e+14</td>\n","      <td>0.127708</td>\n","      <td>0.244082</td>\n","      <td>0.316656</td>\n","      <td>0.550577</td>\n","      <td>0.029351</td>\n","      <td>0.279066</td>\n","      <td>-0.112675</td>\n","      <td>0.121264</td>\n","      <td>-0.302159</td>\n","      <td>-0.185323</td>\n","      <td>-0.350702</td>\n","      <td>-0.102712</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2</td>\n","      <td>0.674908</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.283579</td>\n","      <td>192.100563</td>\n","      <td>0.319200</td>\n","      <td>0.71735</td>\n","      <td>-0.179375</td>\n","      <td>-0.058638</td>\n","      <td>0.204413</td>\n","      <td>0.020714</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.72</td>\n","      <td>3.72</td>\n","      <td>469</td>\n","      <td>466</td>\n","      <td>99.4</td>\n","      <td>1.159</td>\n","      <td>3.4479</td>\n","      <td>0.712650</td>\n","      <td>0.300433</td>\n","      <td>0.000020</td>\n","      <td>1.208437</td>\n","      <td>0.000863</td>\n","      <td>0.564324</td>\n","      <td>0.138205</td>\n","      <td>0.220052</td>\n","      <td>0.043062</td>\n","      <td>-0.146262</td>\n","      <td>-0.999742</td>\n","      <td>-0.042066</td>\n","      <td>-0.998311</td>\n","      <td>-0.195389</td>\n","      <td>-0.220230</td>\n","      <td>0.203368</td>\n","      <td>0.274975</td>\n","      <td>0.056101</td>\n","      <td>-0.395093</td>\n","      <td>-0.058309</td>\n","      <td>-0.275805</td>\n","      <td>UNKNOWN</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>14.63</td>\n","      <td>14.63</td>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>100.0</td>\n","      <td>1.641</td>\n","      <td>NaN</td>\n","      <td>0.042499</td>\n","      <td>0.095228</td>\n","      <td>0.000712</td>\n","      <td>0.087532</td>\n","      <td>0.001297</td>\n","      <td>0.848005</td>\n","      <td>-0.192786</td>\n","      <td>0.553006</td>\n","      <td>0.011435</td>\n","      <td>0.185672</td>\n","      <td>-0.999941</td>\n","      <td>-0.015490</td>\n","      <td>-0.999640</td>\n","      <td>0.184898</td>\n","      <td>-0.555334</td>\n","      <td>-0.177721</td>\n","      <td>0.695521</td>\n","      <td>-0.051359</td>\n","      <td>0.916215</td>\n","      <td>0.043803</td>\n","      <td>-0.699987</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974625</td>\n","      <td>1.002045</td>\n","      <td>0.024838</td>\n","      <td>1.112864</td>\n","      <td>0.319542</td>\n","      <td>-0.080050</td>\n","      <td>-0.002895</td>\n","      <td>0.084850</td>\n","      <td>3.306407e+06</td>\n","      <td>5.247693e+14</td>\n","      <td>0.127611</td>\n","      <td>0.244012</td>\n","      <td>0.311023</td>\n","      <td>0.549365</td>\n","      <td>0.032631</td>\n","      <td>0.279058</td>\n","      <td>-0.118199</td>\n","      <td>0.120049</td>\n","      <td>-0.301125</td>\n","      <td>-0.185385</td>\n","      <td>-0.349773</td>\n","      <td>-0.102711</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2</td>\n","      <td>0.619193</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.524619</td>\n","      <td>1984.965171</td>\n","      <td>0.310583</td>\n","      <td>0.71735</td>\n","      <td>-0.176075</td>\n","      <td>-0.057053</td>\n","      <td>0.198889</td>\n","      <td>0.020571</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.70</td>\n","      <td>3.70</td>\n","      <td>478</td>\n","      <td>476</td>\n","      <td>99.3</td>\n","      <td>1.103</td>\n","      <td>3.4479</td>\n","      <td>0.631000</td>\n","      <td>0.268017</td>\n","      <td>0.000018</td>\n","      <td>1.208442</td>\n","      <td>0.000863</td>\n","      <td>0.564323</td>\n","      <td>-0.010367</td>\n","      <td>0.249339</td>\n","      <td>-0.049027</td>\n","      <td>-0.001410</td>\n","      <td>-0.999242</td>\n","      <td>0.049649</td>\n","      <td>-0.999415</td>\n","      <td>-0.023803</td>\n","      <td>-0.247384</td>\n","      <td>0.035573</td>\n","      <td>0.188556</td>\n","      <td>0.055165</td>\n","      <td>-0.396573</td>\n","      <td>-0.054989</td>\n","      <td>-0.190535</td>\n","      <td>UNKNOWN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>16.59</td>\n","      <td>16.59</td>\n","      <td>18</td>\n","      <td>18</td>\n","      <td>100.0</td>\n","      <td>1.689</td>\n","      <td>NaN</td>\n","      <td>0.042518</td>\n","      <td>0.094869</td>\n","      <td>0.000693</td>\n","      <td>0.087531</td>\n","      <td>0.001295</td>\n","      <td>0.848005</td>\n","      <td>-0.058769</td>\n","      <td>0.545509</td>\n","      <td>-0.108184</td>\n","      <td>0.008024</td>\n","      <td>-0.997869</td>\n","      <td>0.082388</td>\n","      <td>-0.999982</td>\n","      <td>0.060950</td>\n","      <td>-0.547850</td>\n","      <td>-0.010167</td>\n","      <td>0.728109</td>\n","      <td>-0.105330</td>\n","      <td>0.935180</td>\n","      <td>0.060764</td>\n","      <td>-0.730231</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974634</td>\n","      <td>1.002043</td>\n","      <td>0.024837</td>\n","      <td>1.112877</td>\n","      <td>0.319507</td>\n","      <td>-0.080050</td>\n","      <td>-0.002875</td>\n","      <td>0.102885</td>\n","      <td>6.497853e+05</td>\n","      <td>1.597142e+13</td>\n","      <td>0.118361</td>\n","      <td>0.245348</td>\n","      <td>0.270919</td>\n","      <td>0.435801</td>\n","      <td>0.031063</td>\n","      <td>0.280315</td>\n","      <td>-0.158261</td>\n","      <td>0.006527</td>\n","      <td>-0.310922</td>\n","      <td>-0.183766</td>\n","      <td>-0.352052</td>\n","      <td>-0.102419</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2</td>\n","      <td>0.579327</td>\n","      <td>2</td>\n","      <td>-30.000000</td>\n","      <td>-6.270511</td>\n","      <td>404.887321</td>\n","      <td>0.313150</td>\n","      <td>0.71735</td>\n","      <td>-0.175175</td>\n","      <td>-0.057526</td>\n","      <td>0.200537</td>\n","      <td>0.020571</td>\n","      <td>-0.68957</td>\n","      <td>-0.197860</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>3.63</td>\n","      <td>3.63</td>\n","      <td>489</td>\n","      <td>488</td>\n","      <td>99.1</td>\n","      <td>1.104</td>\n","      <td>3.4479</td>\n","      <td>0.564550</td>\n","      <td>0.251450</td>\n","      <td>0.000016</td>\n","      <td>1.208441</td>\n","      <td>0.000863</td>\n","      <td>0.564323</td>\n","      <td>-0.155322</td>\n","      <td>0.217544</td>\n","      <td>-0.016202</td>\n","      <td>0.148930</td>\n","      <td>-0.999550</td>\n","      <td>0.018769</td>\n","      <td>-0.998696</td>\n","      <td>0.104859</td>\n","      <td>-0.217809</td>\n","      <td>-0.098427</td>\n","      <td>0.164164</td>\n","      <td>0.027754</td>\n","      <td>-0.407321</td>\n","      <td>-0.028021</td>\n","      <td>-0.166710</td>\n","      <td>UNKNOWN</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>16.59</td>\n","      <td>16.59</td>\n","      <td>18</td>\n","      <td>18</td>\n","      <td>100.0</td>\n","      <td>1.689</td>\n","      <td>NaN</td>\n","      <td>0.042518</td>\n","      <td>0.094869</td>\n","      <td>0.000693</td>\n","      <td>0.087532</td>\n","      <td>0.001295</td>\n","      <td>0.848005</td>\n","      <td>-0.159724</td>\n","      <td>0.540500</td>\n","      <td>-0.046468</td>\n","      <td>0.141367</td>\n","      <td>-0.999715</td>\n","      <td>0.037042</td>\n","      <td>-0.999366</td>\n","      <td>0.128234</td>\n","      <td>-0.545087</td>\n","      <td>-0.109780</td>\n","      <td>0.722872</td>\n","      <td>-0.057421</td>\n","      <td>0.935103</td>\n","      <td>0.041131</td>\n","      <td>-0.727637</td>\n","      <td>12.0</td>\n","      <td>2.0</td>\n","      <td>0.974630</td>\n","      <td>1.002046</td>\n","      <td>0.024837</td>\n","      <td>1.112879</td>\n","      <td>0.319518</td>\n","      <td>-0.080050</td>\n","      <td>-0.002875</td>\n","      <td>0.096768</td>\n","      <td>7.002269e+05</td>\n","      <td>1.210265e+14</td>\n","      <td>0.112866</td>\n","      <td>0.246049</td>\n","      <td>0.283919</td>\n","      <td>0.498661</td>\n","      <td>0.026102</td>\n","      <td>0.280312</td>\n","      <td>-0.145422</td>\n","      <td>0.069372</td>\n","      <td>-0.316807</td>\n","      <td>-0.183355</td>\n","      <td>-0.354024</td>\n","      <td>-0.102405</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   event_id  time_to_tca mission_id       risk  max_risk_estimate  \\\n","9         2     0.997639          2 -10.816161          -6.601713   \n","10        2     0.955944          2 -10.850473          -6.603452   \n","11        2     0.895711          2 -30.000000          -6.217958   \n","12        2     0.863193          2 -30.000000          -6.271078   \n","13        2     0.815959          2 -30.000000          -6.277448   \n","14        2     0.768235          2 -30.000000          -6.278272   \n","15        2     0.718416          2 -30.000000          -6.283246   \n","16        2     0.674908          2 -30.000000          -6.283579   \n","17        2     0.619193          2 -30.000000          -6.524619   \n","18        2     0.579327          2 -30.000000          -6.270511   \n","\n","    max_risk_scaling  miss_distance  relative_speed  relative_position_r  \\\n","9          13.293159       0.381700         0.71740            -0.289400   \n","10         13.374242       0.382767         0.71740            -0.290275   \n","11        426.808532       0.313083         0.71735            -0.174700   \n","12        181.496778       0.314033         0.71735            -0.175000   \n","13        187.525360       0.316917         0.71735            -0.177475   \n","14        190.090568       0.318950         0.71735            -0.177575   \n","15        188.270059       0.315300         0.71735            -0.178700   \n","16        192.100563       0.319200         0.71735            -0.179375   \n","17       1984.965171       0.310583         0.71735            -0.176075   \n","18        404.887321       0.313150         0.71735            -0.175175   \n","\n","    relative_position_t  relative_position_n  relative_velocity_r  \\\n","9             -0.070069             0.244292             0.022571   \n","10            -0.070336             0.244959             0.022571   \n","11            -0.057516             0.200498             0.020571   \n","12            -0.057690             0.201104             0.020571   \n","13            -0.058246             0.202942             0.020714   \n","14            -0.058596             0.204253             0.020714   \n","15            -0.057932             0.201911             0.020714   \n","16            -0.058638             0.204413             0.020714   \n","17            -0.057053             0.198889             0.020571   \n","18            -0.057526             0.200537             0.020571   \n","\n","    relative_velocity_t  relative_velocity_n t_time_lastob_start  \\\n","9              -0.68960            -0.197855                 1.0   \n","10             -0.68960            -0.197855                 1.0   \n","11             -0.68957            -0.197860                 1.0   \n","12             -0.68957            -0.197860                 1.0   \n","13             -0.68957            -0.197860                 1.0   \n","14             -0.68957            -0.197860                 1.0   \n","15             -0.68957            -0.197860                 1.0   \n","16             -0.68957            -0.197860                 1.0   \n","17             -0.68957            -0.197860                 1.0   \n","18             -0.68957            -0.197860                 1.0   \n","\n","   t_time_lastob_end  t_recommended_od_span  t_actual_od_span  \\\n","9                0.0                   3.92              3.92   \n","10               0.0                   3.86              3.86   \n","11               0.0                   3.85              3.85   \n","12               0.0                   3.83              3.83   \n","13               0.0                   3.72              3.72   \n","14               0.0                   3.69              3.69   \n","15               0.0                   3.63              3.63   \n","16               0.0                   3.72              3.72   \n","17               0.0                   3.70              3.70   \n","18               0.0                   3.63              3.63   \n","\n","    t_obs_available  t_obs_used  t_residuals_accepted  t_weighted_rms  \\\n","9               444         442                  99.4           1.094   \n","10              444         442                  99.4           1.099   \n","11              447         445                  99.4           1.113   \n","12              451         449                  99.4           1.122   \n","13              466         464                  99.5           1.143   \n","14              478         475                  99.5           1.119   \n","15              478         475                  99.5           1.129   \n","16              469         466                  99.4           1.159   \n","17              478         476                  99.3           1.103   \n","18              489         488                  99.1           1.104   \n","\n","    t_rcs_estimate  t_cd_area_over_mass  t_cr_area_over_mass    t_sedr  \\\n","9           3.4505             0.638000             0.156500  0.000019   \n","10          3.4505             0.604200             0.180200  0.000017   \n","11          3.4505             0.575950             0.163283  0.000016   \n","12          3.4479             0.558300             0.168567  0.000016   \n","13          3.4479             0.589075             0.233667  0.000016   \n","14          3.4479             0.590700             0.239400  0.000016   \n","15          3.4479             0.633800             0.265750  0.000017   \n","16          3.4479             0.712650             0.300433  0.000020   \n","17          3.4479             0.631000             0.268017  0.000018   \n","18          3.4479             0.564550             0.251450  0.000016   \n","\n","    t_j2k_sma  t_j2k_ecc  t_j2k_inc    t_ct_r    t_cn_r    t_cn_t  t_crdot_r  \\\n","9    1.208407   0.000860   0.564323 -0.099768  0.357995 -0.122174   0.085472   \n","10   1.208407   0.000861   0.564323 -0.005874  0.360471 -0.036075  -0.002789   \n","11   1.208441   0.000862   0.564323 -0.222621  0.425875 -0.149746   0.206756   \n","12   1.208441   0.000862   0.564323 -0.230120  0.236754 -0.045980   0.222933   \n","13   1.208439   0.000863   0.564323  0.173348  0.318120  0.169947  -0.187696   \n","14   1.208438   0.000862   0.564323 -0.115261  0.255582  0.007605   0.107766   \n","15   1.208439   0.000862   0.564324 -0.188946  0.174655 -0.026595   0.182648   \n","16   1.208437   0.000863   0.564324  0.138205  0.220052  0.043062  -0.146262   \n","17   1.208442   0.000863   0.564323 -0.010367  0.249339 -0.049027  -0.001410   \n","18   1.208441   0.000863   0.564323 -0.155322  0.217544 -0.016202   0.148930   \n","\n","    t_crdot_t  t_crdot_n  t_ctdot_r  t_ctdot_t  t_ctdot_n  t_ctdot_rdot  \\\n","9   -0.999674   0.121504  -0.999114   0.057809  -0.353866     -0.043471   \n","10  -0.999876   0.035870  -0.997255  -0.068114  -0.357012      0.076754   \n","11  -0.999517   0.147289  -0.999479   0.191052  -0.423717     -0.175085   \n","12  -0.999848   0.047067  -0.998058   0.169099  -0.236669     -0.161825   \n","13  -0.999456  -0.170031  -0.999407  -0.207145  -0.321772      0.221392   \n","14  -0.999832  -0.008410  -0.998007   0.052449  -0.257121     -0.044921   \n","15  -0.999807   0.027637  -0.998078   0.127830  -0.174536     -0.121472   \n","16  -0.999742  -0.042066  -0.998311  -0.195389  -0.220230      0.203368   \n","17  -0.999242   0.049649  -0.999415  -0.023803  -0.247384      0.035573   \n","18  -0.999550   0.018769  -0.998696   0.104859  -0.217809     -0.098427   \n","\n","    t_cndot_r  t_cndot_t  t_cndot_n  t_cndot_rdot  t_cndot_tdot c_object_type  \\\n","9   -0.025138   0.087954  -0.430583     -0.088821      0.021409       UNKNOWN   \n","10  -0.027154   0.084268  -0.442266     -0.085037      0.020991       UNKNOWN   \n","11   0.082662   0.017007  -0.405439     -0.018617     -0.083820       UNKNOWN   \n","12   0.104967   0.002330  -0.433469     -0.003761     -0.106536       UNKNOWN   \n","13   0.210865   0.006796  -0.381647     -0.010702     -0.209821       UNKNOWN   \n","14   0.288501  -0.023265  -0.359223      0.022849     -0.288807       UNKNOWN   \n","15   0.286378  -0.055995  -0.380081      0.054933     -0.286015       UNKNOWN   \n","16   0.274975   0.056101  -0.395093     -0.058309     -0.275805       UNKNOWN   \n","17   0.188556   0.055165  -0.396573     -0.054989     -0.190535       UNKNOWN   \n","18   0.164164   0.027754  -0.407321     -0.028021     -0.166710       UNKNOWN   \n","\n","   c_time_lastob_start c_time_lastob_end  c_recommended_od_span  \\\n","9                180.0               2.0                  13.87   \n","10               180.0               2.0                  13.87   \n","11                 1.0               0.0                  14.63   \n","12                 1.0               0.0                  14.63   \n","13                 1.0               0.0                  14.63   \n","14                 2.0               1.0                  14.63   \n","15                 2.0               1.0                  14.63   \n","16                 2.0               1.0                  14.63   \n","17                 1.0               0.0                  16.59   \n","18                 1.0               0.0                  16.59   \n","\n","    c_actual_od_span  c_obs_available  c_obs_used  c_residuals_accepted  \\\n","9              13.87               15          15                 100.0   \n","10             13.87               15          15                 100.0   \n","11             14.63               15          15                 100.0   \n","12             14.63               15          15                 100.0   \n","13             14.63               15          15                 100.0   \n","14             14.63               15          15                 100.0   \n","15             14.63               15          15                 100.0   \n","16             14.63               15          15                 100.0   \n","17             16.59               18          18                 100.0   \n","18             16.59               18          18                 100.0   \n","\n","    c_weighted_rms  c_rcs_estimate  c_cd_area_over_mass  c_cr_area_over_mass  \\\n","9            1.838             NaN             0.040715             0.112876   \n","10           1.838             NaN             0.040715             0.112876   \n","11           1.641             NaN             0.042499             0.095228   \n","12           1.641             NaN             0.042499             0.095228   \n","13           1.641             NaN             0.042499             0.095228   \n","14           1.641             NaN             0.042499             0.095228   \n","15           1.641             NaN             0.042499             0.095228   \n","16           1.641             NaN             0.042499             0.095228   \n","17           1.689             NaN             0.042518             0.094869   \n","18           1.689             NaN             0.042518             0.094869   \n","\n","      c_sedr  c_j2k_sma  c_j2k_ecc  c_j2k_inc    c_ct_r    c_cn_r    c_cn_t  \\\n","9   0.000726   0.087532   0.001367   0.847990 -0.068526  0.636970 -0.038214   \n","10  0.000726   0.087532   0.001367   0.847990 -0.067750  0.636974 -0.038143   \n","11  0.000712   0.087532   0.001297   0.848005  0.025977  0.563595  0.065183   \n","12  0.000712   0.087532   0.001297   0.848005 -0.199922  0.552272  0.010836   \n","13  0.000712   0.087532   0.001297   0.848005 -0.194648  0.552833  0.011237   \n","14  0.000712   0.087532   0.001297   0.848005 -0.193936  0.552834  0.011532   \n","15  0.000712   0.087532   0.001297   0.848005 -0.198442  0.552433  0.010928   \n","16  0.000712   0.087532   0.001297   0.848005 -0.192786  0.553006  0.011435   \n","17  0.000693   0.087531   0.001295   0.848005 -0.058769  0.545509 -0.108184   \n","18  0.000693   0.087532   0.001295   0.848005 -0.159724  0.540500 -0.046468   \n","\n","    c_crdot_r  c_crdot_t  c_crdot_n  c_ctdot_r  c_ctdot_t  c_ctdot_n  \\\n","9    0.064305  -0.999989   0.036762  -0.996314   0.153806  -0.634961   \n","10   0.063521  -0.999989   0.036689  -0.996313   0.153053  -0.634998   \n","11  -0.045196  -0.999602  -0.075887  -0.999774  -0.006036  -0.564147   \n","12   0.192984  -0.999944  -0.014799  -0.999656   0.192663  -0.554558   \n","13   0.187592  -0.999942  -0.015262  -0.999649   0.187206  -0.555115   \n","14   0.186793  -0.999941  -0.015606  -0.999640   0.186100  -0.555175   \n","15   0.191476  -0.999943  -0.014906  -0.999639   0.190277  -0.554820   \n","16   0.185672  -0.999941  -0.015490  -0.999640   0.184898  -0.555334   \n","17   0.008024  -0.997869   0.082388  -0.999982   0.060950  -0.547850   \n","18   0.141367  -0.999715   0.037042  -0.999366   0.128234  -0.545087   \n","\n","    c_ctdot_rdot  c_cndot_r  c_cndot_t  c_cndot_n  c_cndot_rdot  c_cndot_tdot  \\\n","9      -0.149627   0.715984  -0.159057   0.953945      0.156803     -0.723349   \n","10     -0.148865   0.715914  -0.158753   0.953971      0.156495     -0.723302   \n","11      0.025308   0.703561  -0.027022   0.916588      0.007301     -0.706289   \n","12     -0.185664   0.694842  -0.051859   0.916218      0.044474     -0.699265   \n","13     -0.180088   0.695361  -0.051522   0.916218      0.044023     -0.699780   \n","14     -0.178894   0.695308  -0.051279   0.916219      0.043688     -0.699793   \n","15     -0.183248   0.695002  -0.051780   0.916216      0.044368     -0.699527   \n","16     -0.177721   0.695521  -0.051359   0.916215      0.043803     -0.699987   \n","17     -0.010167   0.728109  -0.105330   0.935180      0.060764     -0.730231   \n","18     -0.109780   0.722872  -0.057421   0.935103      0.041131     -0.727637   \n","\n","    t_span  c_span   t_h_apo   t_h_per   c_h_apo   c_h_per  \\\n","9     12.0     2.0  0.974572  1.002050  0.024863  1.112026   \n","10    12.0     2.0  0.974579  1.002044  0.024863  1.112028   \n","11    12.0     2.0  0.974617  1.002059  0.024838  1.112860   \n","12    12.0     2.0  0.974618  1.002058  0.024838  1.112860   \n","13    12.0     2.0  0.974628  1.002045  0.024838  1.112862   \n","14    12.0     2.0  0.974624  1.002047  0.024838  1.112863   \n","15    12.0     2.0  0.974626  1.002048  0.024838  1.112861   \n","16    12.0     2.0  0.974625  1.002045  0.024838  1.112864   \n","17    12.0     2.0  0.974634  1.002043  0.024837  1.112877   \n","18    12.0     2.0  0.974630  1.002046  0.024837  1.112879   \n","\n","    geocentric_latitude   azimuth  elevation  mahalanobis_distance  \\\n","9              0.319779 -0.080044  -0.003155              0.057604   \n","10             0.319783 -0.080044  -0.003155              0.050715   \n","11             0.319517 -0.080050  -0.002875              0.088636   \n","12             0.319521 -0.080050  -0.002875              0.067247   \n","13             0.319533 -0.080050  -0.002895              0.097371   \n","14             0.319541 -0.080050  -0.002895              0.074766   \n","15             0.319526 -0.080050  -0.002895              0.078619   \n","16             0.319542 -0.080050  -0.002895              0.084850   \n","17             0.319507 -0.080050  -0.002875              0.102885   \n","18             0.319518 -0.080050  -0.002875              0.096768   \n","\n","    t_position_covariance_det  c_position_covariance_det  t_sigma_r  \\\n","9                4.110620e+06               2.715773e+18   0.137597   \n","10               1.158624e+07               2.704770e+18   0.137291   \n","11               3.780413e+06               7.644201e+13   0.143460   \n","12               8.405442e+06               5.496282e+14   0.138350   \n","13               1.933940e+06               5.328544e+14   0.131819   \n","14               5.890298e+06               5.200286e+14   0.132682   \n","15               3.582253e+06               5.455925e+14   0.127708   \n","16               3.306407e+06               5.247693e+14   0.127611   \n","17               6.497853e+05               1.597142e+13   0.118361   \n","18               7.002269e+05               1.210265e+14   0.112866   \n","\n","    c_sigma_r  t_sigma_t  c_sigma_t  t_sigma_n  c_sigma_n  t_sigma_rdot  \\\n","9    0.346868   0.312163   0.659368   0.031019   0.336601     -0.117197   \n","10   0.346862   0.343134   0.659245   0.032259   0.336599     -0.086135   \n","11   0.242464   0.300218   0.489541   0.037578   0.279063     -0.129056   \n","12   0.244102   0.331685   0.550805   0.031732   0.279068     -0.097650   \n","13   0.244034   0.291445   0.549841   0.033845   0.279063     -0.137807   \n","14   0.244030   0.323464   0.549077   0.033580   0.279059     -0.105778   \n","15   0.244082   0.316656   0.550577   0.029351   0.279066     -0.112675   \n","16   0.244012   0.311023   0.549365   0.032631   0.279058     -0.118199   \n","17   0.245348   0.270919   0.435801   0.031063   0.280315     -0.158261   \n","18   0.246049   0.283919   0.498661   0.026102   0.280312     -0.145422   \n","\n","    c_sigma_rdot  t_sigma_tdot  c_sigma_tdot  t_sigma_ndot  c_sigma_ndot  F10  \\\n","9       0.230077     -0.291892     -0.081841     -0.346900     -0.067535    0   \n","10      0.229954     -0.291868     -0.081850     -0.346051     -0.067537    0   \n","11      0.060121     -0.286243     -0.186705     -0.345671     -0.102776    0   \n","12      0.121492     -0.291678     -0.185292     -0.346361     -0.102712    0   \n","13      0.120525     -0.297043     -0.185359     -0.344734     -0.102712    0   \n","14      0.119761     -0.296931     -0.185369     -0.349052     -0.102712    0   \n","15      0.121264     -0.302159     -0.185323     -0.350702     -0.102712    0   \n","16      0.120049     -0.301125     -0.185385     -0.349773     -0.102711    0   \n","17      0.006527     -0.310922     -0.183766     -0.352052     -0.102419    0   \n","18      0.069372     -0.316807     -0.183355     -0.354024     -0.102405    0   \n","\n","    F3M  SSN  AP  \n","9     0    0   0  \n","10    0    0   0  \n","11    0    0   0  \n","12    0    0   0  \n","13    0    0   0  \n","14    0    0   0  \n","15    0    0   0  \n","16    0    0   0  \n","17    0    0   0  \n","18    0    0   0  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 151099 entries, 9 to 162633\n","Columns: 103 entries, event_id to AP\n","dtypes: category(7), float64(88), int16(8)\n","memory usage: 106.7 MB\n"]}],"source":["# Import transformed training dataset\n","df = eda.import_cdm_data(os.path.join(cwd,'data','esa-challenge','train_data_transformed.csv'))\n","\n","# Count number of CDMs available per event\n","nb_cdms = df.groupby(['event_id']).count()['time_to_tca'].to_numpy(dtype=np.int16)\n","\n","# Define window size and number of events to forecast\n","seq_length = 5\n","events_to_forecast = 1\n","min_cdms = seq_length + events_to_forecast\n","\n","print(f'Events suitable for training (More than {min_cdms-1} CDMs): {np.sum(nb_cdms>=min_cdms)}'\n","      f' ({np.sum(nb_cdms>=min_cdms)/len(nb_cdms)*100:5.1f}%)')\n","print(f'Time sequences with event_id integrity per feature: {np.sum(nb_cdms[nb_cdms>=min_cdms]-min_cdms)}')\n","\n","# Count number of CDMs per event\n","ts_events  = df[['event_id', 'time_to_tca']].groupby(['event_id']).count().rename(columns={'time_to_tca':'nb_cdms'})\n","\n","# Get events that have a minimum number of CDMs equal to the window_size + events_to_forecast\n","events_filter = list(ts_events[ts_events['nb_cdms']>=min_cdms].index.values)\n","\n","# Redefine DataFrame to contain only events suitable for TSF to save memory\n","df = df[df['event_id'].isin(events_filter)]\n","\n","# Show first data points to explore data types\n","display(df.head(10))\n","df.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Time-Series Forecasting problem"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Converting data from Pandas DataFrame to Time-Series Pytorch Tensors"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["t_cd_area..ver_mass    \tc_cd_area..ver_mass    \t\n","t_cr_area..ver_mass    \tc_cr_area..ver_mass    \t\n","\n","> 100% |██████████| 4/4 | Remaining time: 00:00 (0.65 it/s) > Loading tensors from feature c_cr_area_over_mass ...\n"]}],"source":["# Get input variable features from config file.\n","features = list(dict.keys(config.get_features(**{'cluster':'coefficients', 'continuous':True, 'variable': True})))\n","\n","print(utils.tabular_list(features))\n","\n","# Check if file containing tensors is available in the data folder and load it\n","# tsf_tensors = torch.load(filepath) if os.path.exists(filepath) else {}\n","tsf_tensors = {}\n","\n","# Iterate over all features to get the time series subsets\n","pb_features = utils.progressbar(iterations = range(len(features)), desc_loc='right')\n","for f in pb_features.iterations:\n","\n","    # Initialize list of tensors for feature f\n","    feature = features[f]\n","    \n","    # Get filename and filepath and check if it already exists\n","    filename = f'ts_{seq_length}-{events_to_forecast}-{feature}.pt'\n","    filepath = os.path.join(cwd,'data','tensors', filename)\n","\n","    if os.path.exists(filepath):\n","        # If tensors already exists, load them.\n","        description = f'> Loading tensors from feature {feature} ...'\n","        pb_features.refresh(i = f+1, description = description)\n","        \n","        tsf_tensors[feature] = torch.load(filepath)\n","    else:\n","\n","        # If tensor do not exist for a feature, initialize and get all time-series\n","        tsf_tensors[feature] = []\n","\n","        # Get full sequence from dataset and convert it to a tensor.\n","        feature_dtype = str(df[feature].dtype).lower()\n","        data = df[feature].to_numpy(dtype=feature_dtype)\n","\n","        for e, event_id in enumerate(events_filter):\n","\n","            # Print progress bar\n","            subprogress = (e+1)/len(events_filter)*100\n","            description = f'> Extracting sequences of time-series from feature {feature:<30s}  (Progress: {subprogress:5.1f}%)'\n","            pb_features.refresh(i = f+1, description = description)\n","\n","            # Get full sequence from dataset and convert it to a tensor.\n","            event_filter = (df['event_id']==event_id)\n","            full_seq = torch.nan_to_num(torch.FloatTensor(data[event_filter]))\n","\n","            # Add Time-Series subsets from full sequence tensor and add it to the list for the feature f\n","            tsf_tensors[feature] = tsf_tensors[feature] + rnn.event_ts_sets(full_seq, seq_length)\n","\n","        # Save tensors containing all Time-Series subsets for training organised by feature.\n","        description = f'Saving tensors with sequences of time-series into external file {\".\"*(len(description)-64)}'\n","        torch.save(tsf_tensors[feature], os.path.join(cwd,'data','tensors', filename))\n","        pb_features.refresh(i = f+1, description = description)\n","\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Get Inputs and Outputs tensors from Time-Series"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["> 100% |██████████| 104099/104099 | Remaining time: 00:00 (37117.73 it/s) > Getting training and target tensors ...\n"]}],"source":["# Get inputs and outputs in the right shape to feed the LSTM model\n","tsf_data = rnn.tsf_iotensors(tsf_tensors, features, seq_length)\n","inputs  = tsf_data['inputs']  # Tensor with shape (batch_size, seq_length, input_size)\n","outputs = tsf_data['outputs'] # Tensor with shape (batch_size, input_size)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.6042, 0.1802, 0.0407, 0.1129],\n","        [0.5760, 0.1633, 0.0425, 0.0952],\n","        [0.5583, 0.1686, 0.0425, 0.0952],\n","        [0.5891, 0.2337, 0.0425, 0.0952],\n","        [0.5907, 0.2394, 0.0425, 0.0952]])\n","tensor([0.6338, 0.2657, 0.0425, 0.0952])\n"]}],"source":["batch = 1\n","print(inputs[batch, :, :]) # Inputs shape is (batch_size, seq_length, input_size)\n","print(outputs[batch])      # Outputs shape is (batch_size, input_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Training and test split on inputs/outputs tensors"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 78000 sequences \n"," Test size: 26099 sequences\n"]}],"source":["train_split = 0.75\n","\n","batch_size, seq_length, inputs_size = inputs.size()\n","\n","train_size = int(((batch_size*train_split)//1000)*1000)\n","test_size = batch_size - train_size\n","\n","print(f'Train size: {train_size} sequences \\n Test size: {test_size} sequences')\n","\n","X_train = inputs[:train_size-test_size]\n","X_test  = inputs[train_size-test_size:train_size]\n","\n","y_train = outputs[:train_size-test_size]\n","y_test  = outputs[train_size-test_size:train_size]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Neural network class definition and instanciation of the model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Definition of `LSTMtoLinear()` class"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class LSTMtoLinear(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, output_size, \n","                 num_layers = 1, batch_first = True, bidirectional = False):\n","        super(LSTMtoLinear, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.num_layers = num_layers\n","        self.batch_first = batch_first\n","        self.bidirectional = bidirectional\n","\n","        self.model = nn.ModuleDict({\n","            'lstm': nn.LSTM(\n","                input_size = input_size,    # 45, see the data definition\n","                hidden_size = hidden_size,  # Can vary\n","                num_layers = num_layers,\n","                batch_first = batch_first,\n","            ),\n","            'linear': nn.Linear(\n","                in_features = hidden_size * (2 if bidirectional else 1),\n","                out_features = output_size,\n","            )\n","        })\n","\n","    def forward(self, inputs):\n","\n","        seq_length, batch_size, input_size = inputs.size()\n","       \n","        # Data is fed to the LSTM\n","        #  - input: tensor shape depending on batch_first parameter:\n","        #       if False -> (seq_length, batch_size, input_size)\n","        #       if True  -> (batch_size, seq_length, input_size)\n","        #\n","        #  - output: tensor shape depending on batch_first parameter:\n","        #       if False -> (seq_length, batch_size, D * hidden_size) \n","        #       if True  -> (batch_size, seq_length, D * hidden_size)\n","        #       where D=2 if bidirectional=True, D=1 otherwise.\n","\n","        # The output from LSTM is one prediction per each input timestep, \n","        # even though only the last timestep’s output is required.\n","        lstm_out, lstm_states = self.model['lstm'](inputs)\n","        # print(f'LSTM output size = {lstm_out.size()}')\n","\n","        # Data is fed to the Linear layer:\n","        #  - input: (*, input_size), where * means any number of dimensions including None.\n","        #  - output: (*, output_size), where all but the last dimension are the same shape as the input.\n","        linear_out = self.model['linear'](lstm_out[:, -1, :])\n","        # print(f'Linear Regression output size = {linear_out.size()}')\n","\n","        # # The prediction utilizing the whole sequence is the last one\n","        # y_pred = linear_out[:, :, -1].unsqueeze(-1)\n","        # print(f'y_pred={y_pred.size()}')\n","\n","        return linear_out"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### RNN model: `MultiEventPropagation` class"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","# Define Multivariate LSTM network class\n","class MultiEventPropagation(nn.Module):\n","    def __init__(self, input_size, output_size, layers, p = 0.5):\n","        super(MultiEventPropagation, self).__init__()\n","        self.input_size = input_size    # Number of input features\n","        self.output_size = output_size  # Number of outputs\n","        self.layers = layers    # Number of recurrent (stacked) layers\n","    \n","        layerlist = []\n","        n_in = input_size\n","        for l, layer in enumerate(layers):\n","\n","            # On layer l, which contains n_neurons, perform the following operations:\n","            # 1. Apply Linear or LSTM to Linear neural network layer\n","\n","            layer_type = layer['type']\n","            layer_neurons = layer['neurons']\n","            kwargs = layer.get('lstm_config', {'num_layers':1, 'batch_first': True})\n","\n","            if layer_type == 'linear':\n","                layerlist.append(nn.Linear(n_in, layer_neurons['output']))\n","            else:\n","\n","                layerlist.append(LSTMtoLinear(input_size = n_in, \n","                                            hidden_size = layer_neurons['hidden'],\n","                                            output_size = layer_neurons['output'],\n","                                            num_layers = kwargs.get('num_layers', 1),\n","                                            batch_first = kwargs.get('batch_first', True)))\n","\n","            # 2. Apply Hyperbolic Tangent activation function (al(z))\n","            layerlist.append(nn.Tanh())\n","            \n","            # 3. Normalize data using the n_neurons\n","            # layerlist.append(nn.BatchNorm1d(layer_neurons['output']))\n","            \n","            # 4. Cancel out a random proportion p of the neurons to avoid overfitting\n","            layerlist.append(nn.Dropout(p))\n","\n","            # 5. Set new number of input features n_in for the next layer l+1.\n","            n_in = layer_neurons['output']\n","\n","        # Set the last layer of the list which corresponds to the final output\n","        layerlist.append(nn.Linear(n_in, output_size))\n","\n","        # Instantiate layers as a Neural Network sequential task\n","        self.layers = nn.Sequential(*layerlist)\n","        \n","    def forward(self, inputs): \n","\n","        # Process all data points with the layers functions (sequential of operations)\n","        outputs = self.layers(inputs)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["MultiEventPropagation(\n","  (layers): Sequential(\n","    (0): Linear(in_features=4, out_features=200, bias=True)\n","    (1): Tanh()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): LSTMtoLinear(\n","      (model): ModuleDict(\n","        (lstm): LSTM(200, 400, batch_first=True)\n","        (linear): Linear(in_features=400, out_features=400, bias=True)\n","      )\n","    )\n","    (4): Tanh()\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=400, out_features=200, bias=True)\n","    (7): Tanh()\n","    (8): Dropout(p=0.5, inplace=False)\n","    (9): Linear(in_features=200, out_features=4, bias=True)\n","  )\n",")\n"]}],"source":["# Instanciate model with required inputs.\n","torch.manual_seed(42)\n","\n","layers = [{'type':'linear', 'neurons':{'output':200}},\n","          {'type':'lstm', 'neurons':{'output':400, 'hidden': 400}},\n","          {'type':'linear', 'neurons':{'output':200}}]\n","\n","model = MultiEventPropagation(input_size = len(features),\n","                              output_size = len(features),\n","                              layers = layers)\n","\n","# Define criterion, optimizer, and scheduler (dynamic learning rate adapter)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1)\n","\n","# Print model\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model training"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([401, 5, 400])\n","Linear Regression output size = torch.Size([401, 400])\n","LSTM output size = torch.Size([51901, 5, 400])\n","Linear Regression output size = torch.Size([51901, 400])\n","LSTM output size = torch.Size([26099, 5, 400])\n","Linear Regression output size = torch.Size([26099, 400])\n","Epoch 0: Train RMSE     0.0600, Test RMSE     0.0598\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n","LSTM output size = torch.Size([500, 5, 400])\n","Linear Regression output size = torch.Size([500, 400])\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m X_batch, y_batch \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m---> 10\u001b[0m     y_pred \u001b[39m=\u001b[39m model(X_batch)\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[16], line 51\u001b[0m, in \u001b[0;36mMultiEventPropagation.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs): \n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[39m# Process all data points with the layers functions (sequential of operations)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(inputs)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch.utils.data as torch_data\n"," \n","\n","loader = torch_data.DataLoader(torch_data.TensorDataset(X_train, y_train), shuffle=True, batch_size=500)\n"," \n","n_epochs = 2000\n","for epoch in range(n_epochs):\n","    model.train()\n","    for X_batch, y_batch in loader:\n","        y_pred = model(X_batch)\n","        loss = criterion(y_pred, y_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    if epoch % 100 != 0: continue\n","\n","    model.eval()\n","    with torch.no_grad():\n","        y_pred = model(X_train)\n","        train_rmse = np.sqrt(criterion(y_pred, y_train))\n","        y_pred = model(X_test)\n","        test_rmse = np.sqrt(criterion(y_pred, y_test))\n","    print(f'Epoch {epoch}: Train RMSE {train_rmse:10.4f}, Test RMSE {test_rmse:10.4f}')"]},{"cell_type":"code","execution_count":73,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Extracting sequences of time-series ...:   0%|          | 0/100 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([500, 5, 4])\n","LSTM output size = torch.Size([5, 500, 400])\n","Transformed LSTM output size = torch.Size([5, 400])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"TypeError","evalue":"forward() takes 2 positional arguments but 3 were given","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[73], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Initialize hidden state and compute outputs\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(inputs_b\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> 36\u001b[0m forecast \u001b[39m=\u001b[39m model(inputs_b) \n\u001b[1;32m     38\u001b[0m \u001b[39m# Compute loss using the outputs for the batch b and store values in array\u001b[39;00m\n\u001b[1;32m     39\u001b[0m loss \u001b[39m=\u001b[39m criterion(forecast, outputs_b)  \n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[70], line 51\u001b[0m, in \u001b[0;36mMultiEventPropagation.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs): \n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[39m# Process all data points with the layers functions (sequential of operations)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(inputs)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[69], line 52\u001b[0m, in \u001b[0;36mLSTMtoLinear.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTransformed LSTM output size = \u001b[39m\u001b[39m{\u001b[39;00mlstm_out\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39m# Data is fed to the Linear layer:\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m#  - input: (*, input_size), where * means any number of dimensions including None.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m#  - output: (*, output_size), where all but the last dimension are the same shape as the input.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m linear_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel[\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m](batch_size, lstm_out)\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLinear Regression output size = \u001b[39m\u001b[39m{\u001b[39;00mlinear_out\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39m# # The prediction utilizing the whole sequence is the last one\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# y_pred = linear_out[:, :, -1].unsqueeze(-1)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# print(f'y_pred={y_pred.size()}')\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"]}],"source":["import time\n","start_time = time.time()\n","\n","\n","epochs = 100\n","batch_size = 500\n","\n","# Initialize array of inf values for losses\n","losses = np.ones(epochs)*np.inf\n","lr = np.zeros(epochs)\n","\n","loader = torch_data.DataLoader(torch_data.TensorDataset(X_train, y_train), shuffle = True, batch_size = batch_size)\n","\n","# Iterate over all remaining features to get the time series subsets\n","pb_epochs = utils.progressbar(range(epochs), description='Training Conjunction Event Propagation model ...', desc_loc='right')\n","\n","for e in pb_epochs.iterations:\n","    \n","    lr[e] = rnn.get_lr(optimizer)\n","    model.train()\n","\n","    # Train model by passing n batches depending on the batch_size\n","    batches = range(0, len(X_train), batch_size)\n","    batch_losses = np.ones(len(batches))*np.inf\n","\n","    for b, batch in enumerate(batches):\n","        \n","        # Get inputs and outputs for batch b\n","        upper_limit = batch+batch_size if batch+batch_size<len(inputs_train) else len(inputs_train)\n","        X_batch = X_train[batch:upper_limit]\n","        y_batch = y_train[batch:upper_limit]  \n","        \n","        # Reset Gradient from the optimizer (hidden and cell states)\n","        optimizer.zero_grad()\n","        \n","        # Initialize hidden state and compute outputs\n","        y_pred = model(X_batch) \n","        \n","        # Compute loss using the outputs for the batch b and store values in array\n","        loss = criterion(y_pred, y_batch)  \n","        batch_losses[b] = loss.detach().numpy()\n","        \n","        # Back propagate loss and adjust parameters of the optimizer\n","        loss.backward()\n","        optimizer.step()\n","\n","    losses[e] = batch_losses[-1] \n","\n","    # Update progress bar\n","    description = f'Training Conjunction Event Propagation model | Learning rate = {lr[e]:.2e} | MSE loss = {losses[e]:10.8f} -> sqrt(MSE) = {np.sqrt(losses[e]):10.8f}')\n","    pb_epochs.refresh(e+1, description = description)\n","\n","    #if lr[e]<1e-6: break\n","    scheduler.step(loss)\n","        \n","print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 600x300 with 2 Axes>"]},"metadata":{"image/png":{"height":313,"width":619}},"output_type":"display_data"}],"source":["fig, ax1 = plt.subplots(figsize=(6,3))\n","plt.title(r'Model training', fontsize=12)\n","ax2 = ax1.twinx()\n","ax1.set_xlim(0, epochs)\n","ax1.set_ylim(0,np.max(losses))\n","ax2.set_ylim(0,np.max(lr)*1.25)\n","ax1.plot(range(0,epochs), losses, 'r-', label=r'MSE loss')\n","ax2.plot(range(0,epochs), lr, 'g-', label=r'Learning rate')\n","\n","ax1.set_xlabel(r'Epochs')\n","ax1.set_ylabel(r'MSE error', color='r')\n","ax2.set_ylabel(r'Learning rate', color='g')\n","ax1.grid(True, linestyle='--', c='grey')\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['t_cd_area_over_mass', 't_cr_area_over_mass', 'c_cd_area_over_mass', 'c_cr_area_over_mass']\n","tensor([[0.1924, 0.0000, 0.0005, 0.0000],\n","        [0.1961, 0.0000, 0.0007, 0.0000],\n","        [0.1980, 0.0000, 0.0009, 0.0000],\n","        [0.1919, 0.0000, 0.0010, 0.0000],\n","        [0.1943, 0.0000, 0.0009, 0.0000]])\n","tensor([ 1.9661e-01,  1.9821e-02,  7.9015e-04, -1.4404e-04],\n","       grad_fn=<SelectBackward0>)\n","tensor([0.1953, 0.0000, 0.0007, 0.0000])\n","MSE loss on test dataset: 0.00286999\n"]}],"source":["model.init_hidden(inputs_test[:1].size(0))\n","forecast_val = model(inputs_test[:1])\n","print(features)\n","print(inputs_test[0])\n","print(forecast_val[0])\n","print(outputs_test[0])\n","\n","# Evaluate entire dataset\n","with torch.no_grad():\n","\n","    model.init_hidden(inputs_test.size(0))\n","    forecast_val = model(inputs_test)\n","    loss = criterion(forecast_val, outputs_test)\n","print(f'MSE loss on test dataset: {loss:.8f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Tests"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### RNN with a single LSTM-Linear layer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define Multivariate LSTM network class\n","class EventPropagation(nn.Module):\n","    def __init__(self,input_size, hidden_size, output_size, seq_length, num_layers=1):\n","        super(EventPropagation, self).__init__()\n","        self.input_size = input_size    # Number of input features\n","        self.hidden_size = hidden_size  # Number of hidden neurons\n","        self.output_size = output_size  # Number of outputs\n","        self.num_layers = num_layers    # Number of recurrent (stacked) layers\n","        self.seq_length = seq_length\n","    \n","        self.lstm = nn.LSTM(input_size = self.input_size, \n","                            hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers,\n","                            batch_first = True)\n","        # according to pytorch docs LSTM output is \n","        # (batch_size,seq_len, num_directions * hidden_size)\n","        # when considering batch_first = True\n","        self.linear = nn.Linear(self.hidden_size*self.seq_length, \n","                                self.output_size)\n","    \n","    \n","    def forward(self, inputs):        \n","        batch_size, seq_length, input_size = inputs.size()\n","        \n","        lstm_out, lstm_states = self.lstm(inputs)\n","        # lstm_out(with batch_first = True) is \n","        # (batch_size,seq_len,num_directions * hidden_size)\n","        # for following linear layer we want to keep batch_size dimension and merge rest       \n","        # .contiguous() -> solves tensor compatibility error\n","        inputs = lstm_out.contiguous().view(input_size,-1)\n","        outputs = self.linear(inputs)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["EventPropagation(\n","  (lstm): LSTM(4, 100, batch_first=True)\n","  (linear): Linear(in_features=500, out_features=4, bias=True)\n",")\n"]}],"source":["# Instanciate model with required inputs.\n","torch.manual_seed(42)\n","model = rnn.EventPropagation(input_size = len(features), \n","                         hidden_size = 100,\n","                         output_size = len(features),\n","                         seq_length = seq_length)\n","\n","# Define criterion, optimizer, and scheduler (dynamic learning rate adapter)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1)\n","\n","# Print model\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# A batch is in shape [batches, batch_size, seq_length, input_size]  \n","x=torch.Size([128, 30, 12, 45])\n","y=torch.Size([128, 30, 1, 1])\n","\n","class MockupModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.model = nn.ModuleDict({\n","            'lstm': nn.LSTM(\n","                input_size=x_features,    # 45, see the data definition\n","                hidden_size=hidden_size,  # Can vary\n","            ),\n","            'linear': nn.Linear(\n","                in_features=hidden_size,\n","                out_features=1)\n","        })\n","\n","    def forward(self, x):\n","\n","        # From [batches, batch_size, seq_length, input_size]\n","        # to [seq_length, batch_size, input_size]\n","        x = x.view(x_seq_len, -1, x_features)\n","       \n","        # Data is fed to the LSTM\n","        out, _ = self.model['lstm'](x)\n","        print(f'lstm output={out.size()}')\n","\n","        # From [seq len, batch, num_directions * hidden_size]\n","        # to [batches, seqs, seq_len,prediction]\n","        out = out.view(x_batches, x_seqs, x_seq_len, -1)\n","        print(f'transformed output={out.size()}')\n","\n","        # Data is fed to the Linear layer\n","        out = self.model['linear'](out)\n","        print(f'linear output={out.size()}')\n","\n","        # The prediction utilizing the whole sequence is the last one\n","        y_pred = out[:, :, -1].unsqueeze(-1)\n","        print(f'y_pred={y_pred.size()}')\n","\n","        return y_pred"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":2}
