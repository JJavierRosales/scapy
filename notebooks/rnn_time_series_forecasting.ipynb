{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.- Import standard libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/jjrr/Documents/SCA-Project/scalib\n"]}],"source":["# Import custom libraries from local folder.\n","from importlib import reload\n","import os\n","import sys\n","sys.path.append(\"..\")\n","\n","# Import nn module from torch to replicate kessler tool\n","import torch.nn as nn\n","\n","# Import utils library containing miscellaneous functions/classes\n","from scalib import utils\n","\n","# Import library to import Kelvins challlenge data\n","from scalib.eda import kelvins_challenge_events\n","\n","# Import SCALIB modules for NN development\n","import scalib.xnn as xnn            # NN models\n","import scalib.cells as cell         # RNN cell architectures\n","import scalib.layers as layers      # RNN layers\n","\n","# Set overall seed for reproducibility\n","utils.seed(1)\n","\n","# Import matplotlib library and setup environment for plots\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","from matplotlib import rc\n","\n","# Set rendering parameters to use TeX font if not working on Juno app.\n","if not '/private/var/' in utils.cwd:\n","    rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 11})\n","    rc('text', usetex=True)\n","\n","print(utils.cwd)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.- Data preparation"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Kelvins Challenge dataset imported from external file (162634 entries):\n","/Users/jjrr/Documents/SCA-Project/scalib/data/esa-challenge/train_data.csv\n","\n","KELVINS DATASET IMPORT:\n","| Progress                     |    Time     | Iters/sec | Comments\n","| 100% |██████████| (2000/2000)| 00h:00m:10s |  196.59   | Dataset imported (2000 events).         \n","\n","CONJUNCTION EVENTS DATASET -> PANDAS DATAFRAME:\n","| Progress                     |    Time     | Iters/sec | Comments\n","| 100% |██████████| (2000/2000)| 00h:00m:30s |   64.92   | Pandas DataFrame saved.                                 \n","\n","Test data: ConjunctionEventsDataset(Events:100 | Number of CDMs per event: 1 (min), 22 (max), 12.80 (mean))\n","Training and validation data: ConjunctionEventsDataset(Events:1900 | Number of CDMs per event: 1 (min), 22 (max), 13.45 (mean))\n"]}],"source":["#As an example, we first show the case in which the data comes from the Kelvins competition.\n","#For this, we built a specific converter that takes care of the conversion from Kelvins format\n","#to standard CDM format (the data can be downloaded at https://kelvins.esa.int/collision-avoidance-challenge/data/):\n","filepath = os.path.join(utils.cwd,'data','esa-challenge','train_data.csv')\n","\n","# Get ConjunctionEventsDataset object \n","events = kelvins_challenge_events(filepath,\n","            drop_features = ['c_rcs_estimate', 't_rcs_estimate'], \n","            num_events = 2000)\n","\n","# Get features to train the model.\n","nn_features = events.common_features(only_numeric=True)\n","\n","# Define input and output size of the RNN model.\n","input_size = len(nn_features)\n","output_size = len(nn_features)\n","\n","# Split data into a test set (5% of the total number of events)\n","len_test_set=int(0.05*len(events))\n","\n","# Get Events to test model: used to compute the error the model would have in \n","# run-mode.\n","events_test=events[-len_test_set:]\n","print('\\nTest data:', events_test)\n","\n","# Get events used for training and validation:\n","# - Training set: Used to train the model and backpropagate the loss.\n","# - Validation set: Used to compute the loss so that hyperparameters can be \n","#   adjusted.\n","events_train_and_val=events[:-len_test_set]\n","print('Training and validation data:', events_train_and_val)"]},{"cell_type":"markdown","metadata":{},"source":["## 3.- Recurrent Neural Network model configuration"]},{"cell_type":"markdown","metadata":{},"source":["### 3.1.- RNN layer and cell architecture definition"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Initialize dictionary of pytorch modules\n","networks = nn.ModuleDict({})\n","\n","# Initialize parameters\n","num_layers = 2      # Number of stacked LSTM layers\n","dropout = 0.2       # Dropout probability between layers\n","hidden_size = 264   # Hidden size\n","batch_size = 15"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Initialize LSTM architecture using custom cell\n","networks.update({'kessler':\n","                 nn.ModuleDict({'lstm': nn.LSTM(input_size = input_size,\n","                                        batch_first = True,\n","                                        hidden_size = hidden_size,\n","                                        num_layers = num_layers,\n","                                        dropout = dropout),\n","                                'dropout': nn.Dropout(p = dropout),\n","                                'relu': nn.ReLU(),\n","                                'linear': nn.Linear(hidden_size, output_size)\n","                                })\n","                })"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.2.- LSTM layer with *Vanilla* cell architecture ([Kessler](https://github.com/kesslerlib/kessler.git)'s configuration)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize LSTM architecture using custom cell\n","networks.update({'lstm_vanilla':\n","                 nn.ModuleDict({'lstm': layers.LSTM(input_size = input_size,\n","                                         batch_first = True, \n","                                         hidden_size = hidden_size, \n","                                         num_layers = num_layers,\n","                                         dropout = dropout,\n","                                         cell = cell.LSTM_Vanilla),\n","                                'dropout': nn.Dropout(p = dropout),\n","                                'relu': nn.ReLU(),\n","                                'linear': nn.Linear(hidden_size, output_size)\n","                                })\n","                })"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.3.- LSTM layer with *SLIMX* cell architecture (*x* = 1, 2, or 3)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jjrr/Documents/SCA-Project/scalib/notebooks/../scalib/layers.py:195: UserWarning: Dropout parameter in LSTM class adds dropout layers after all but last recurrent layer. It expects num_layers greater > 1, but got num_layers = 1.\n","  warnings.warn(\n"]}],"source":["# Initialize LSTM architecture using SLIMx cell architecture. \n","# Three options are available by passing the extra parameter slim_version\n","# 1 - Gates contain hidden states weights and bias (Wh + b). Default value.\n","# 2 - Gates contain hidden states weights only (Wh).\n","# 3 - Gates contain learnable bias only (b).\n","reload(cell)\n","reload(layers)\n","hidden_sizes = [313, 314, 897]\n","for v in [1, 2, 3]:\n","    networks.update({f'lstm_slim{v}':\n","                     nn.ModuleDict({'lstm': layers.LSTM(input_size = input_size, \n","                                            hidden_size = hidden_sizes[v-1],\n","                                            cell = cell.LSTM_SLIMX, \n","                                            num_layers = num_layers if v < 3 \\\n","                                                                    else 1,\n","                                            dropout = dropout,\n","                                            **dict(slim_version = v)),\n","                                    'dropout': nn.Dropout(p = dropout),\n","                                    'relu': nn.ReLU(),\n","                                    'linear': nn.Linear(hidden_sizes[v-1], output_size)\n","                                    })\n","                    })"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.4.- LSTM layer with no *X* gate (NXG) in cell architecture (*X* = Input, Forget or Output)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Initialize LSTM architecture using NXG cell architecture (one of the \n","# information gates within the cell is cancelled out.\n","hidden_sizes = [306, 306, 306] \n","for g, gate in enumerate(['input', 'output', 'forget']):\n","    networks.update({f'lstm_n{gate[0]}g':\n","                     nn.ModuleDict({'lstm': layers.LSTM(input_size = input_size, \n","                                            hidden_size = hidden_sizes[g],\n","                                            cell = cell.LSTM_NXG, \n","                                            num_layers = num_layers,\n","                                            dropout = dropout,\n","                                            **dict(drop_gate = gate)),\n","                                    'dropout': nn.Dropout(p = dropout),\n","                                    'relu': nn.ReLU(),\n","                                    'linear': nn.Linear(hidden_sizes[g], output_size)\n","                                    })\n","                    })"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.5.- LSTM layer with no activation function at *X* gate (NXGAF) in cell architecture (*X* = Input, Forget, Output, or Cell)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Initialize LSTM architecture using NXG cell architecture (one of the \n","# information gates within the cell is cancelled out. \n","for gate in ['input', 'output', 'forget', 'cell']:\n","    networks.update({f'lstm_n{gate[0]}gaf':\n","                     nn.ModuleDict({'lstm': layers.LSTM(input_size = input_size, \n","                                            hidden_size = hidden_size,\n","                                            cell = cell.LSTM_NXGAF, \n","                                            num_layers = num_layers,\n","                                            dropout = dropout,\n","                                            **dict(naf_gate = gate)),\n","                                    'dropout': nn.Dropout(p = dropout),\n","                                    'relu': nn.ReLU(),\n","                                    'linear': nn.Linear(hidden_size, output_size)\n","                                    }),\n","                    \n","                    })"]},{"cell_type":"markdown","metadata":{},"source":["#### 3.1.6.- LSTM-Attention-LSTM layer"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Initialize Self-Attention network with LSTM Vanilla encoders\n","attention_hs = 230\n","networks.update({f'lstm_attn-vanilla':\n","                 nn.ModuleDict({'lstm_encoder': \n","                                    layers.LSTM(\n","                                        input_size = input_size,\n","                                        hidden_size = attention_hs,\n","                                        cell = cell.LSTM_Vanilla),\n","                                'attention': \n","                                    layers.SelfAttentionLayer(\n","                                        input_size = attention_hs),\n","                                'lstm_decoder': \n","                                    layers.LSTM(\n","                                        input_size = attention_hs, \n","                                        hidden_size = attention_hs,\n","                                        cell = cell.LSTM_Vanilla),\n","                                'dropout': nn.Dropout(p = dropout),\n","                                'relu': nn.ReLU(),\n","                                'linear': nn.Linear(attention_hs, output_size)\n","                                })\n","                })\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Initialize Self-Attention network with SLIM3 encoders\n","attention_hs = 354\n","networks.update({f'lstm_attn-slim3':\n","                 nn.ModuleDict({'lstm_encoder': \n","                                    layers.LSTM(\n","                                        input_size = input_size,\n","                                        hidden_size = attention_hs,\n","                                        cell = cell.LSTM_SLIMX,\n","                                        **dict(slim_version=3)),\n","                                'attention': \n","                                    layers.SelfAttentionLayer(\n","                                        input_size = attention_hs),\n","                                'lstm_decoder': \n","                                    layers.LSTM(\n","                                        input_size = attention_hs, \n","                                        hidden_size = attention_hs,\n","                                        cell = cell.LSTM_SLIMX,\n","                                        **dict(slim_version=3)),\n","                                'dropout': nn.Dropout(p = dropout),\n","                                'relu': nn.ReLU(),\n","                                'linear': nn.Linear(attention_hs, output_size)\n","                                })\n","                })"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Initialize LSTM architecture using custom cell\n","reload(layers)\n","reload(cell)\n","networks.update({'gru_vanilla':\n","                 nn.ModuleDict({'gru': layers.GRU(input_size = input_size,\n","                                         batch_first = True, \n","                                         hidden_size = 307, \n","                                         num_layers = num_layers,\n","                                         dropout = dropout,\n","                                         cell = cell.GRU_Vanilla),\n","                                'dropout': nn.Dropout(p = dropout),\n","                                'relu': nn.ReLU(),\n","                                'linear': nn.Linear(hidden_size, output_size)\n","                                })\n","                })"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2.- Model instanciation"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" Parameters  | Network\n","---------------------------\n","  927,762    | KESSLER\n","  927,762    | LSTM_VANILLA\n","  925,607    | LSTM_SLIM1\n","  929,506    | LSTM_SLIM2\n","  926,667    | LSTM_SLIM3\n","  927,246    | LSTM_NIG\n","  927,246    | LSTM_NOG\n","  927,246    | LSTM_NFG\n","  927,762    | LSTM_NIGAF\n","  927,762    | LSTM_NOGAF\n","  927,762    | LSTM_NFGAF\n","  927,762    | LSTM_NCGAF\n","  926,966    | LSTM_ATTN-VANILLA\n","  928,254    | LSTM_ATTN-SLIM3\n","  928,359    | GRU_VANILLA\n"]}],"source":["# Print number of parameters per model\n","print('{:^12} | {:<}\\n{}'.format('Parameters', 'Network', '-'*27))\n","for name, network in networks.items():\n","\n","    # Get number of parameters in the model.\n","    num_params = sum(p.numel() for p in network.parameters())\n","\n","    print('{:^12,} | {:<}'.format(num_params, name.upper()))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model filename: cef_gru_vanilla_hs307_nl2_bs15\n"]}],"source":["import torch\n","reload(xnn)\n","\n","# Print model.\n","network_name = 'gru_vanilla'\n","network = networks[network_name]\n","\n","# print(f'\\n{network}\\n')\n","\n","# Initialize model.\n","model = xnn.ConjunctionEventForecaster(layers = network, features = nn_features)\n","\n","# Iterate over network to get the parameters.\n","for name, layer in network.items():\n","    if network_name.split('_')[0] in name:\n","        layer.hidden_size\n","        filename = f'cef_{network_name}_' + \\\n","                   f'hs{layer.hidden_size}_' + \\\n","                   f'nl{layer.num_layers}_bs{batch_size}'\n","        break\n","        \n","filepath_model = os.path.join(utils.cwd, 'models', filename)\n","\n","torch.save(network, filepath_model + '_network')\n","\n","print(f'Model filename: {filename}')"]},{"cell_type":"markdown","metadata":{},"source":["### Model training"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of learnable parameters of the model: 928,359\n","\n","CONJUNCTION EVENTS DATASET -> PANDAS DATAFRAME:\n","| Progress                     |    Time     | Iters/sec | Comments\n","| 100% |██████████| (1900/1900)| 00h:00m:00s | 93957.32  | Pandas DataFrame saved.                                 \n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (5733x307 and 264x66)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mlearn(events_train_and_val, epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, lr \u001b[39m=\u001b[39;49m \u001b[39m1e-3\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m batch_size, \n\u001b[1;32m      3\u001b[0m             device \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m, valid_proportion \u001b[39m=\u001b[39;49m \u001b[39m0.15\u001b[39;49m, num_workers \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m             event_samples_for_stats \u001b[39m=\u001b[39;49m \u001b[39m2000\u001b[39;49m, filepath_model\u001b[39m=\u001b[39;49mfilepath_model \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_parameters\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m             epoch_step_checkpoint\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n","File \u001b[0;32m~/Documents/SCA-Project/scalib/notebooks/../scalib/xnn.py:447\u001b[0m, in \u001b[0;36mConjunctionEventForecaster.learn\u001b[0;34m(self, event_set, epochs, lr, batch_size, device, valid_proportion, num_workers, event_samples_for_stats, filepath_model, epoch_step_checkpoint, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset(batch_size)\n\u001b[1;32m    443\u001b[0m \u001b[39m# Forecast next CDMs of the mini-batch using the inputs. The \u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m# model also requires a second parameter with the number of \u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# CDMs per event object in order to pack padded sequences to \u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m# optimize computation.\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(inputs, event_lengths)\n\u001b[1;32m    449\u001b[0m \u001b[39m# Compute loss using the criterion and add it to the array.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(output, target)\n","File \u001b[0;32m~/Documents/SCA-Project/scalib/notebooks/../scalib/xnn.py:845\u001b[0m, in \u001b[0;36mConjunctionEventForecaster.forward\u001b[0;34m(self, x, x_lengths)\u001b[0m\n\u001b[1;32m    837\u001b[0m         x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden[module_name] \u001b[39m=\u001b[39m module(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden[module_name])\n\u001b[1;32m    839\u001b[0m         \u001b[39m# # Pads a packed batch of variable length sequences from LSTM layer.\u001b[39;00m\n\u001b[1;32m    840\u001b[0m         \u001b[39m# x, _ = pad_packed_sequence(sequence = x, \u001b[39;00m\n\u001b[1;32m    841\u001b[0m         \u001b[39m#                            batch_first = module.batch_first, \u001b[39;00m\n\u001b[1;32m    842\u001b[0m         \u001b[39m#                            total_length = x_length_max)\u001b[39;00m\n\u001b[1;32m    843\u001b[0m         \n\u001b[1;32m    844\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 845\u001b[0m         x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m    847\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/homebrew/anaconda3/envs/irpenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5733x307 and 264x66)"]}],"source":["# Start training\n","model.learn(events_train_and_val, epochs = 10, lr = 1e-3, batch_size = batch_size, \n","            device = 'cpu', valid_proportion = 0.15, num_workers = 4,\n","            event_samples_for_stats = 2000, filepath_model=filepath_model + '_parameters',\n","            epoch_step_checkpoint=5)"]},{"cell_type":"markdown","metadata":{},"source":["##### Training vs validation loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot MSE loss throughout iterations.\n","model.plot_loss(filepath = os.path.join(utils.cwd, 'images', filename + '.pdf'), \n","                log_scale = True, \n","                plot_lr = False)"]},{"cell_type":"markdown","metadata":{},"source":["##### Conjunction event forecasting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Take a single event from test dataset and remove the last CDM.\n","event_idx = 2\n","event = events_test[event_idx]\n","event_beginning = event[0:len(event)-1]\n","\n","# Print information about the event to forecast.\n","print(f'Forecasting next CDM from previous {len(event)} CDM(s)...')\n","\n","# Predict the evolution of the conjunction event until TCA or the number of CDMs\n","# is max_length.\n","event_evolution = model.predict_event(event = event_beginning, \n","                                      num_samples = 10, \n","                                      max_length = 14)\n","\n","# List of features to predict.\n","features = ['RELATIVE_SPEED', 'MISS_DISTANCE', 'OBJECT1_CT_T']\n","\n","# Plot prediction in red\n","axs = event_evolution.plot_features(features = features, return_axs = True, \n","                                    linewidth = 0.1, color = 'red', alpha=0.33, \n","                                    label = 'Prediction')\n","#and the ground truth value in blue:\n","event.plot_features(features=features, axs=axs, label='Actual', legend = True)"]},{"cell_type":"markdown","metadata":{},"source":["## Comparing models performance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","def get_models_performance(folderpath:str, referential_network:str):\n","\n","    # Initialize dictionary to keep models performance.\n","    data = dict()\n","\n","    # Iterate over all elements in the folder\n","    for f in os.listdir(folderpath):\n","\n","        # Check if it is a model file.\n","        if not os.path.isfile(os.path.join(folderpath, f)) or not \\\n","            (f[:3]=='cef' and f.split('_')[-1]=='network'): continue\n","        \n","        filename = '_'.join(f.split('_')[1:-1])\n","\n","        network = torch.load(os.path.join(folderpath, f'cef_{filename}_network'))\n","        parameters = torch.load(os.path.join(folderpath, f'cef_{filename}_parameters'))\n","\n","        results = parameters['learn_results']\n","\n","        data[filename.upper()] = [sum(p.numel() for p in network.parameters()),\n","                                    results['validation_loss'][-1],\n","                                    results['total_iterations'][-1],\n","                                    results['epoch'][-1],\n","                                    results['learning_rate'][-1]]\n","\n","    columns = ['PARAMETERS', 'LOSS', 'ITERATIONS', 'EPOCHS', 'LEARNING_RATE']\n","    df = pd.DataFrame.from_dict(columns = columns, data=data, orient='index')\n","    if referential_network is not None:\n","        ref_loss = df.loc[referential_network]['LOSS']\n","        df['RELATIVE_LOSS'] = (df['LOSS']-ref_loss)/ref_loss\n","\n","    return df\n","\n","df = get_models_performance(os.path.join(utils.cwd,'models'),\n","                               referential_network='LSTM_VANILLA_HS264_NL2_BS15')\n","\n","# Save results in a CSV file.\n","df.to_csv('rnn_networks_performance.csv')\n","\n","display(df.sort_values(by=['LOSS'], ascending=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reload(xnn)\n","import matplotlib.pyplot as plt\n","folderpath = os.path.join(utils.cwd, 'models')\n","\n","# Iterate over all elements in the folder\n","fig, ax = plt.subplots(figsize=(6,3))\n","for f in os.listdir(folderpath):\n","\n","    # Check if it is a model file.\n","    if not os.path.isfile(os.path.join(folderpath, f)) or not \\\n","        (f[:3]=='cef' and f.split('_')[-1]=='network'): continue\n","    \n","    filename = '_'.join(f.split('_')[1:-1])\n","\n","    network = torch.load(os.path.join(folderpath, f'cef_{filename}_network'))\n","    \n","    model = xnn.ConjunctionEventForecaster(layers = network, features = nn_features)\n","    model.load(os.path.join(folderpath, f'cef_{filename}_parameters'))\n","    model.plot_loss(validation_only = True, log_scale = False, ax = ax, \n","                    label = ' '.join(filename.split('_')[:2]).upper())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":2}
