{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import standard libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"import torch\nimport torch.nn as nn  # we'll use this a lot going forward!\nimport torch.nn.functional as F\n\nimport numpy as np\nimport warnings\n\n# Import matplotlib library and setup environment for plots\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nfrom matplotlib import pyplot as plt, rc\n\n# Import json library and create function to format dictionaries.\nimport json\nformat_json = lambda x: json.dumps(x, indent=4)\n\n# Import pandas and set pandas DataFrame visualization parameters\nfrom IPython.display import display\nimport pandas as pd\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\n# Set rendering parameters to use TeX font if not working on Juno app.\nfrom pathlib import Path\nimport os\nif not '/private/var/' in os.getcwd():\n    rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 11})\n    rc('text', usetex=True)\n    \n# Get current working directory path for the tool parent folder and print it.\nparent_folder = 'Tool'\ncwd = str(Path(os.getcwd()[:os.getcwd().index(parent_folder)+len(parent_folder)]))\nprint('Parent working directory: %s' % cwd)\n","execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Parent working directory: /Users/jjrr/Documents/SCA-Project/Tool\n"}]},{"metadata":{},"cell_type":"markdown","source":"## Import user defined libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import custom libraries from local folder.\nimport sys\nsys.path.append(\"..\")\n\nfrom library.irplib import utils, eda, config, sdg\nfrom library.irplib import rnn","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"### Import training dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import transformed training dataset\ndf = eda.import_cdm_data(os.path.join(cwd,'data','esa-challenge','train_data_transformed.csv'))\n\n# Count number of CDMs available per event\nnb_cdms = df.groupby(['event_id']).count()['time_to_tca'].to_numpy(dtype=np.int16)\n\n# Define window size and number of events to forecast\nwindow_size = 5\nevents_to_forecast = 1\nmin_cdms = window_size + events_to_forecast\n\nprint(f'Events suitable for training (More than {min_cdms-1} CDMs): {np.sum(nb_cdms>=min_cdms)}'\n      f' ({np.sum(nb_cdms>=min_cdms)/len(nb_cdms)*100:5.1f}%)')\nprint(f'Time sequences with event_id integrity per feature: {np.sum(nb_cdms[nb_cdms>=min_cdms]-min_cdms)}')\n\n# Count number of CDMs per event\nts_events  = df[['event_id', 'time_to_tca']].groupby(['event_id']).count().rename(columns={'time_to_tca':'nb_cdms'})\n\n# Get events that have a minimum number of CDMs equal to the window_size + events_to_forecast\nevents_filter = list(ts_events[ts_events['nb_cdms']>=min_cdms].index.values)\n\n# Redefine DataFrame to contain only events suitable for TSF to save memory\ndf = df[df['event_id'].isin(events_filter)]\n\n# Show first data points to explore data types\ndisplay(df.head(10))\ndf.info()","execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Events suitable for training (More than 5 CDMs): 9400 ( 71.5%)\nTime sequences with event_id integrity per feature: 94699\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>time_to_tca</th>\n      <th>mission_id</th>\n      <th>risk</th>\n      <th>max_risk_estimate</th>\n      <th>max_risk_scaling</th>\n      <th>miss_distance</th>\n      <th>relative_speed</th>\n      <th>relative_position_r</th>\n      <th>relative_position_t</th>\n      <th>relative_position_n</th>\n      <th>relative_velocity_r</th>\n      <th>relative_velocity_t</th>\n      <th>relative_velocity_n</th>\n      <th>t_time_lastob_start</th>\n      <th>t_time_lastob_end</th>\n      <th>t_recommended_od_span</th>\n      <th>t_actual_od_span</th>\n      <th>t_obs_available</th>\n      <th>t_obs_used</th>\n      <th>t_residuals_accepted</th>\n      <th>t_weighted_rms</th>\n      <th>t_rcs_estimate</th>\n      <th>t_cd_area_over_mass</th>\n      <th>t_cr_area_over_mass</th>\n      <th>t_sedr</th>\n      <th>t_j2k_sma</th>\n      <th>t_j2k_ecc</th>\n      <th>t_j2k_inc</th>\n      <th>t_ct_r</th>\n      <th>t_cn_r</th>\n      <th>t_cn_t</th>\n      <th>t_crdot_r</th>\n      <th>t_crdot_t</th>\n      <th>t_crdot_n</th>\n      <th>t_ctdot_r</th>\n      <th>t_ctdot_t</th>\n      <th>t_ctdot_n</th>\n      <th>t_ctdot_rdot</th>\n      <th>t_cndot_r</th>\n      <th>t_cndot_t</th>\n      <th>t_cndot_n</th>\n      <th>t_cndot_rdot</th>\n      <th>t_cndot_tdot</th>\n      <th>c_object_type</th>\n      <th>c_time_lastob_start</th>\n      <th>c_time_lastob_end</th>\n      <th>c_recommended_od_span</th>\n      <th>c_actual_od_span</th>\n      <th>c_obs_available</th>\n      <th>c_obs_used</th>\n      <th>c_residuals_accepted</th>\n      <th>c_weighted_rms</th>\n      <th>c_rcs_estimate</th>\n      <th>c_cd_area_over_mass</th>\n      <th>c_cr_area_over_mass</th>\n      <th>c_sedr</th>\n      <th>c_j2k_sma</th>\n      <th>c_j2k_ecc</th>\n      <th>c_j2k_inc</th>\n      <th>c_ct_r</th>\n      <th>c_cn_r</th>\n      <th>c_cn_t</th>\n      <th>c_crdot_r</th>\n      <th>c_crdot_t</th>\n      <th>c_crdot_n</th>\n      <th>c_ctdot_r</th>\n      <th>c_ctdot_t</th>\n      <th>c_ctdot_n</th>\n      <th>c_ctdot_rdot</th>\n      <th>c_cndot_r</th>\n      <th>c_cndot_t</th>\n      <th>c_cndot_n</th>\n      <th>c_cndot_rdot</th>\n      <th>c_cndot_tdot</th>\n      <th>t_span</th>\n      <th>c_span</th>\n      <th>t_h_apo</th>\n      <th>t_h_per</th>\n      <th>c_h_apo</th>\n      <th>c_h_per</th>\n      <th>geocentric_latitude</th>\n      <th>azimuth</th>\n      <th>elevation</th>\n      <th>mahalanobis_distance</th>\n      <th>t_position_covariance_det</th>\n      <th>c_position_covariance_det</th>\n      <th>t_sigma_r</th>\n      <th>c_sigma_r</th>\n      <th>t_sigma_t</th>\n      <th>c_sigma_t</th>\n      <th>t_sigma_n</th>\n      <th>c_sigma_n</th>\n      <th>t_sigma_rdot</th>\n      <th>c_sigma_rdot</th>\n      <th>t_sigma_tdot</th>\n      <th>c_sigma_tdot</th>\n      <th>t_sigma_ndot</th>\n      <th>c_sigma_ndot</th>\n      <th>F10</th>\n      <th>F3M</th>\n      <th>SSN</th>\n      <th>AP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>6.983474</td>\n      <td>2</td>\n      <td>-10.816161</td>\n      <td>-6.601713</td>\n      <td>13.293159</td>\n      <td>22902.0</td>\n      <td>14348.0</td>\n      <td>-1157.6</td>\n      <td>-6306.2</td>\n      <td>21986.3</td>\n      <td>15.8</td>\n      <td>-13792.0</td>\n      <td>-3957.1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.92</td>\n      <td>3.92</td>\n      <td>444</td>\n      <td>442</td>\n      <td>99.4</td>\n      <td>1.094</td>\n      <td>3.4505</td>\n      <td>3.042086</td>\n      <td>0.924980</td>\n      <td>-10.894082</td>\n      <td>7158.394530</td>\n      <td>0.000860</td>\n      <td>98.523094</td>\n      <td>-0.099768</td>\n      <td>0.357995</td>\n      <td>-0.122174</td>\n      <td>0.085472</td>\n      <td>-0.999674</td>\n      <td>0.121504</td>\n      <td>-0.999114</td>\n      <td>0.057809</td>\n      <td>-0.353866</td>\n      <td>-0.043471</td>\n      <td>-0.025138</td>\n      <td>0.087954</td>\n      <td>-0.430583</td>\n      <td>-0.088821</td>\n      <td>0.021409</td>\n      <td>UNKNOWN</td>\n      <td>180.0</td>\n      <td>2.0</td>\n      <td>13.87</td>\n      <td>13.87</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.838</td>\n      <td>NaN</td>\n      <td>1.579769</td>\n      <td>2.227246</td>\n      <td>-7.228422</td>\n      <td>7168.396928</td>\n      <td>0.001367</td>\n      <td>69.717278</td>\n      <td>-0.068526</td>\n      <td>0.636970</td>\n      <td>-0.038214</td>\n      <td>0.064305</td>\n      <td>-0.999989</td>\n      <td>0.036762</td>\n      <td>-0.996314</td>\n      <td>0.153806</td>\n      <td>-0.634961</td>\n      <td>-0.149627</td>\n      <td>0.715984</td>\n      <td>-0.159057</td>\n      <td>0.953945</td>\n      <td>0.156803</td>\n      <td>-0.723349</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.417082</td>\n      <td>774.097978</td>\n      <td>800.056782</td>\n      <td>780.463075</td>\n      <td>63.955771</td>\n      <td>-16.008858</td>\n      <td>-0.063092</td>\n      <td>115.208802</td>\n      <td>15.229084</td>\n      <td>42.445608</td>\n      <td>2.201549</td>\n      <td>5.549886</td>\n      <td>4.994608</td>\n      <td>10.549895</td>\n      <td>0.496310</td>\n      <td>5.385613</td>\n      <td>-1.875151</td>\n      <td>3.681239</td>\n      <td>-4.670266</td>\n      <td>-1.309462</td>\n      <td>-5.550399</td>\n      <td>-1.080559</td>\n      <td>73</td>\n      <td>77</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2</td>\n      <td>6.691611</td>\n      <td>2</td>\n      <td>-10.850473</td>\n      <td>-6.603452</td>\n      <td>13.374242</td>\n      <td>22966.0</td>\n      <td>14348.0</td>\n      <td>-1161.1</td>\n      <td>-6330.2</td>\n      <td>22046.3</td>\n      <td>15.8</td>\n      <td>-13792.0</td>\n      <td>-3957.1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.86</td>\n      <td>3.86</td>\n      <td>444</td>\n      <td>442</td>\n      <td>99.4</td>\n      <td>1.099</td>\n      <td>3.4505</td>\n      <td>2.880922</td>\n      <td>1.065057</td>\n      <td>-10.960651</td>\n      <td>7158.394561</td>\n      <td>0.000861</td>\n      <td>98.523097</td>\n      <td>-0.005874</td>\n      <td>0.360471</td>\n      <td>-0.036075</td>\n      <td>-0.002789</td>\n      <td>-0.999876</td>\n      <td>0.035870</td>\n      <td>-0.997255</td>\n      <td>-0.068114</td>\n      <td>-0.357012</td>\n      <td>0.076754</td>\n      <td>-0.027154</td>\n      <td>0.084268</td>\n      <td>-0.442266</td>\n      <td>-0.085037</td>\n      <td>0.020991</td>\n      <td>UNKNOWN</td>\n      <td>180.0</td>\n      <td>2.0</td>\n      <td>13.87</td>\n      <td>13.87</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.838</td>\n      <td>NaN</td>\n      <td>1.579769</td>\n      <td>2.227246</td>\n      <td>-7.228422</td>\n      <td>7168.397641</td>\n      <td>0.001367</td>\n      <td>69.717278</td>\n      <td>-0.067750</td>\n      <td>0.636974</td>\n      <td>-0.038143</td>\n      <td>0.063521</td>\n      <td>-0.999989</td>\n      <td>0.036689</td>\n      <td>-0.996313</td>\n      <td>0.153053</td>\n      <td>-0.634998</td>\n      <td>-0.148865</td>\n      <td>0.715914</td>\n      <td>-0.158753</td>\n      <td>0.953971</td>\n      <td>0.156495</td>\n      <td>-0.723302</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.420510</td>\n      <td>774.094612</td>\n      <td>800.057080</td>\n      <td>780.464203</td>\n      <td>63.956674</td>\n      <td>-16.008858</td>\n      <td>-0.063092</td>\n      <td>101.429474</td>\n      <td>16.265328</td>\n      <td>42.441549</td>\n      <td>2.196657</td>\n      <td>5.549796</td>\n      <td>5.490139</td>\n      <td>10.547926</td>\n      <td>0.516145</td>\n      <td>5.385589</td>\n      <td>-1.378157</td>\n      <td>3.679266</td>\n      <td>-4.669890</td>\n      <td>-1.309606</td>\n      <td>-5.536811</td>\n      <td>-1.080597</td>\n      <td>73</td>\n      <td>77</td>\n      <td>27</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2</td>\n      <td>6.269979</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.217958</td>\n      <td>426.808532</td>\n      <td>18785.0</td>\n      <td>14347.0</td>\n      <td>-698.8</td>\n      <td>-5176.4</td>\n      <td>18044.8</td>\n      <td>14.4</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.85</td>\n      <td>3.85</td>\n      <td>447</td>\n      <td>445</td>\n      <td>99.4</td>\n      <td>1.113</td>\n      <td>3.4505</td>\n      <td>2.746222</td>\n      <td>0.965072</td>\n      <td>-11.022029</td>\n      <td>7158.407962</td>\n      <td>0.000862</td>\n      <td>98.523100</td>\n      <td>-0.222621</td>\n      <td>0.425875</td>\n      <td>-0.149746</td>\n      <td>0.206756</td>\n      <td>-0.999517</td>\n      <td>0.147289</td>\n      <td>-0.999479</td>\n      <td>0.191052</td>\n      <td>-0.423717</td>\n      <td>-0.175085</td>\n      <td>0.082662</td>\n      <td>0.017007</td>\n      <td>-0.405439</td>\n      <td>-0.018617</td>\n      <td>-0.083820</td>\n      <td>UNKNOWN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.395887</td>\n      <td>0.001297</td>\n      <td>69.718437</td>\n      <td>0.025977</td>\n      <td>0.563595</td>\n      <td>0.065183</td>\n      <td>-0.045196</td>\n      <td>-0.999602</td>\n      <td>-0.075887</td>\n      <td>-0.999774</td>\n      <td>-0.006036</td>\n      <td>-0.564147</td>\n      <td>0.025308</td>\n      <td>0.703561</td>\n      <td>-0.027022</td>\n      <td>0.916588</td>\n      <td>0.007301</td>\n      <td>-0.706289</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.439755</td>\n      <td>774.102169</td>\n      <td>799.554662</td>\n      <td>780.963112</td>\n      <td>63.903391</td>\n      <td>-16.009902</td>\n      <td>-0.057504</td>\n      <td>177.272242</td>\n      <td>15.145344</td>\n      <td>31.967553</td>\n      <td>2.295355</td>\n      <td>3.879431</td>\n      <td>4.803485</td>\n      <td>7.832651</td>\n      <td>0.601252</td>\n      <td>4.465008</td>\n      <td>-2.064897</td>\n      <td>0.961933</td>\n      <td>-4.579886</td>\n      <td>-2.987280</td>\n      <td>-5.530733</td>\n      <td>-1.644409</td>\n      <td>71</td>\n      <td>77</td>\n      <td>23</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2</td>\n      <td>6.042352</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.271078</td>\n      <td>181.496778</td>\n      <td>18842.0</td>\n      <td>14347.0</td>\n      <td>-700.0</td>\n      <td>-5192.1</td>\n      <td>18099.4</td>\n      <td>14.4</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.83</td>\n      <td>3.83</td>\n      <td>451</td>\n      <td>449</td>\n      <td>99.4</td>\n      <td>1.122</td>\n      <td>3.4479</td>\n      <td>2.662064</td>\n      <td>0.996299</td>\n      <td>-11.058486</td>\n      <td>7158.407846</td>\n      <td>0.000862</td>\n      <td>98.523108</td>\n      <td>-0.230120</td>\n      <td>0.236754</td>\n      <td>-0.045980</td>\n      <td>0.222933</td>\n      <td>-0.999848</td>\n      <td>0.047067</td>\n      <td>-0.998058</td>\n      <td>0.169099</td>\n      <td>-0.236669</td>\n      <td>-0.161825</td>\n      <td>0.104967</td>\n      <td>0.002330</td>\n      <td>-0.433469</td>\n      <td>-0.003761</td>\n      <td>-0.106536</td>\n      <td>UNKNOWN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.396232</td>\n      <td>0.001297</td>\n      <td>69.718436</td>\n      <td>-0.199922</td>\n      <td>0.552272</td>\n      <td>0.010836</td>\n      <td>0.192984</td>\n      <td>-0.999944</td>\n      <td>-0.014799</td>\n      <td>-0.999656</td>\n      <td>0.192663</td>\n      <td>-0.554558</td>\n      <td>-0.185664</td>\n      <td>0.694842</td>\n      <td>-0.051859</td>\n      <td>0.916218</td>\n      <td>0.044474</td>\n      <td>-0.699265</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.439933</td>\n      <td>774.101759</td>\n      <td>799.555083</td>\n      <td>780.963381</td>\n      <td>63.904169</td>\n      <td>-16.009902</td>\n      <td>-0.057504</td>\n      <td>134.494670</td>\n      <td>15.944390</td>\n      <td>33.940263</td>\n      <td>2.213602</td>\n      <td>3.905626</td>\n      <td>5.306960</td>\n      <td>8.812877</td>\n      <td>0.507719</td>\n      <td>4.465087</td>\n      <td>-1.562397</td>\n      <td>1.943879</td>\n      <td>-4.666855</td>\n      <td>-2.964674</td>\n      <td>-5.541783</td>\n      <td>-1.643388</td>\n      <td>71</td>\n      <td>77</td>\n      <td>23</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>5.711716</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.277448</td>\n      <td>187.525360</td>\n      <td>19015.0</td>\n      <td>14347.0</td>\n      <td>-709.9</td>\n      <td>-5242.1</td>\n      <td>18264.8</td>\n      <td>14.5</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.72</td>\n      <td>3.72</td>\n      <td>466</td>\n      <td>464</td>\n      <td>99.5</td>\n      <td>1.143</td>\n      <td>3.4479</td>\n      <td>2.808804</td>\n      <td>1.381067</td>\n      <td>-11.012908</td>\n      <td>7158.407228</td>\n      <td>0.000863</td>\n      <td>98.523107</td>\n      <td>0.173348</td>\n      <td>0.318120</td>\n      <td>0.169947</td>\n      <td>-0.187696</td>\n      <td>-0.999456</td>\n      <td>-0.170031</td>\n      <td>-0.999407</td>\n      <td>-0.207145</td>\n      <td>-0.321772</td>\n      <td>0.221392</td>\n      <td>0.210865</td>\n      <td>0.006796</td>\n      <td>-0.381647</td>\n      <td>-0.010702</td>\n      <td>-0.209821</td>\n      <td>UNKNOWN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.396846</td>\n      <td>0.001297</td>\n      <td>69.718436</td>\n      <td>-0.194648</td>\n      <td>0.552833</td>\n      <td>0.011237</td>\n      <td>0.187592</td>\n      <td>-0.999942</td>\n      <td>-0.015262</td>\n      <td>-0.999649</td>\n      <td>0.187206</td>\n      <td>-0.555115</td>\n      <td>-0.180088</td>\n      <td>0.695361</td>\n      <td>-0.051522</td>\n      <td>0.916218</td>\n      <td>0.044023</td>\n      <td>-0.699780</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.444957</td>\n      <td>774.095498</td>\n      <td>799.555077</td>\n      <td>780.964615</td>\n      <td>63.906532</td>\n      <td>-16.009902</td>\n      <td>-0.057903</td>\n      <td>194.741883</td>\n      <td>14.475070</td>\n      <td>33.909269</td>\n      <td>2.109108</td>\n      <td>3.904538</td>\n      <td>4.663119</td>\n      <td>8.797461</td>\n      <td>0.541527</td>\n      <td>4.465003</td>\n      <td>-2.204913</td>\n      <td>1.928403</td>\n      <td>-4.752686</td>\n      <td>-2.965744</td>\n      <td>-5.515744</td>\n      <td>-1.643389</td>\n      <td>71</td>\n      <td>77</td>\n      <td>23</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2</td>\n      <td>5.377642</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.278272</td>\n      <td>190.090568</td>\n      <td>19137.0</td>\n      <td>14347.0</td>\n      <td>-710.3</td>\n      <td>-5273.6</td>\n      <td>18382.8</td>\n      <td>14.5</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.69</td>\n      <td>3.69</td>\n      <td>478</td>\n      <td>475</td>\n      <td>99.5</td>\n      <td>1.119</td>\n      <td>3.4479</td>\n      <td>2.816552</td>\n      <td>1.414954</td>\n      <td>-11.019932</td>\n      <td>7158.406819</td>\n      <td>0.000862</td>\n      <td>98.523103</td>\n      <td>-0.115261</td>\n      <td>0.255582</td>\n      <td>0.007605</td>\n      <td>0.107766</td>\n      <td>-0.999832</td>\n      <td>-0.008410</td>\n      <td>-0.998007</td>\n      <td>0.052449</td>\n      <td>-0.257121</td>\n      <td>-0.044921</td>\n      <td>0.288501</td>\n      <td>-0.023265</td>\n      <td>-0.359223</td>\n      <td>0.022849</td>\n      <td>-0.288807</td>\n      <td>UNKNOWN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.397203</td>\n      <td>0.001297</td>\n      <td>69.718436</td>\n      <td>-0.193936</td>\n      <td>0.552834</td>\n      <td>0.011532</td>\n      <td>0.186793</td>\n      <td>-0.999941</td>\n      <td>-0.015606</td>\n      <td>-0.999640</td>\n      <td>0.186100</td>\n      <td>-0.555175</td>\n      <td>-0.178894</td>\n      <td>0.695308</td>\n      <td>-0.051279</td>\n      <td>0.916219</td>\n      <td>0.043688</td>\n      <td>-0.699793</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.443328</td>\n      <td>774.096310</td>\n      <td>799.555148</td>\n      <td>780.965258</td>\n      <td>63.908209</td>\n      <td>-16.009902</td>\n      <td>-0.057903</td>\n      <td>149.531226</td>\n      <td>15.588817</td>\n      <td>33.884905</td>\n      <td>2.122911</td>\n      <td>3.904485</td>\n      <td>5.175424</td>\n      <td>8.785229</td>\n      <td>0.537283</td>\n      <td>4.464941</td>\n      <td>-1.692445</td>\n      <td>1.916182</td>\n      <td>-4.750897</td>\n      <td>-2.965900</td>\n      <td>-5.584831</td>\n      <td>-1.643389</td>\n      <td>70</td>\n      <td>77</td>\n      <td>11</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>5.028915</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.283246</td>\n      <td>188.270059</td>\n      <td>18918.0</td>\n      <td>14347.0</td>\n      <td>-714.8</td>\n      <td>-5213.9</td>\n      <td>18172.0</td>\n      <td>14.5</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.63</td>\n      <td>3.63</td>\n      <td>478</td>\n      <td>475</td>\n      <td>99.5</td>\n      <td>1.129</td>\n      <td>3.4479</td>\n      <td>3.022060</td>\n      <td>1.570693</td>\n      <td>-10.953841</td>\n      <td>7158.407343</td>\n      <td>0.000862</td>\n      <td>98.523115</td>\n      <td>-0.188946</td>\n      <td>0.174655</td>\n      <td>-0.026595</td>\n      <td>0.182648</td>\n      <td>-0.999807</td>\n      <td>0.027637</td>\n      <td>-0.998078</td>\n      <td>0.127830</td>\n      <td>-0.174536</td>\n      <td>-0.121472</td>\n      <td>0.286378</td>\n      <td>-0.055995</td>\n      <td>-0.380081</td>\n      <td>0.054933</td>\n      <td>-0.286015</td>\n      <td>UNKNOWN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.395801</td>\n      <td>0.001297</td>\n      <td>69.718436</td>\n      <td>-0.198442</td>\n      <td>0.552433</td>\n      <td>0.010928</td>\n      <td>0.191476</td>\n      <td>-0.999943</td>\n      <td>-0.014906</td>\n      <td>-0.999639</td>\n      <td>0.190277</td>\n      <td>-0.554820</td>\n      <td>-0.183248</td>\n      <td>0.695002</td>\n      <td>-0.051780</td>\n      <td>0.916216</td>\n      <td>0.044368</td>\n      <td>-0.699527</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.444058</td>\n      <td>774.096628</td>\n      <td>799.553665</td>\n      <td>780.963937</td>\n      <td>63.905222</td>\n      <td>-16.009902</td>\n      <td>-0.057903</td>\n      <td>157.237923</td>\n      <td>15.091503</td>\n      <td>33.932893</td>\n      <td>2.043324</td>\n      <td>3.905318</td>\n      <td>5.066500</td>\n      <td>8.809227</td>\n      <td>0.469616</td>\n      <td>4.465049</td>\n      <td>-1.802808</td>\n      <td>1.940231</td>\n      <td>-4.834537</td>\n      <td>-2.965160</td>\n      <td>-5.611228</td>\n      <td>-1.643385</td>\n      <td>70</td>\n      <td>77</td>\n      <td>11</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2</td>\n      <td>4.724355</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.283579</td>\n      <td>192.100563</td>\n      <td>19152.0</td>\n      <td>14347.0</td>\n      <td>-717.5</td>\n      <td>-5277.4</td>\n      <td>18397.2</td>\n      <td>14.5</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.72</td>\n      <td>3.72</td>\n      <td>469</td>\n      <td>466</td>\n      <td>99.4</td>\n      <td>1.159</td>\n      <td>3.4479</td>\n      <td>3.398029</td>\n      <td>1.775686</td>\n      <td>-10.828875</td>\n      <td>7158.406303</td>\n      <td>0.000863</td>\n      <td>98.523117</td>\n      <td>0.138205</td>\n      <td>0.220052</td>\n      <td>0.043062</td>\n      <td>-0.146262</td>\n      <td>-0.999742</td>\n      <td>-0.042066</td>\n      <td>-0.998311</td>\n      <td>-0.195389</td>\n      <td>-0.220230</td>\n      <td>0.203368</td>\n      <td>0.274975</td>\n      <td>0.056101</td>\n      <td>-0.395093</td>\n      <td>-0.058309</td>\n      <td>-0.275805</td>\n      <td>UNKNOWN</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>14.63</td>\n      <td>14.63</td>\n      <td>15</td>\n      <td>15</td>\n      <td>100.0</td>\n      <td>1.641</td>\n      <td>NaN</td>\n      <td>1.649021</td>\n      <td>1.879016</td>\n      <td>-7.247312</td>\n      <td>7168.396820</td>\n      <td>0.001297</td>\n      <td>69.718436</td>\n      <td>-0.192786</td>\n      <td>0.553006</td>\n      <td>0.011435</td>\n      <td>0.185672</td>\n      <td>-0.999941</td>\n      <td>-0.015490</td>\n      <td>-0.999640</td>\n      <td>0.184898</td>\n      <td>-0.555334</td>\n      <td>-0.177721</td>\n      <td>0.695521</td>\n      <td>-0.051359</td>\n      <td>0.916215</td>\n      <td>0.043803</td>\n      <td>-0.699987</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.443569</td>\n      <td>774.095036</td>\n      <td>799.554287</td>\n      <td>780.965353</td>\n      <td>63.908418</td>\n      <td>-16.009902</td>\n      <td>-0.057903</td>\n      <td>169.699821</td>\n      <td>15.011373</td>\n      <td>33.893980</td>\n      <td>2.041773</td>\n      <td>3.904195</td>\n      <td>4.976370</td>\n      <td>8.789843</td>\n      <td>0.522089</td>\n      <td>4.464934</td>\n      <td>-1.891187</td>\n      <td>1.920781</td>\n      <td>-4.817995</td>\n      <td>-2.966167</td>\n      <td>-5.596369</td>\n      <td>-1.643383</td>\n      <td>70</td>\n      <td>77</td>\n      <td>11</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2</td>\n      <td>4.334354</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.524619</td>\n      <td>1984.965171</td>\n      <td>18635.0</td>\n      <td>14347.0</td>\n      <td>-704.3</td>\n      <td>-5134.8</td>\n      <td>17900.0</td>\n      <td>14.4</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.70</td>\n      <td>3.70</td>\n      <td>478</td>\n      <td>476</td>\n      <td>99.3</td>\n      <td>1.103</td>\n      <td>3.4479</td>\n      <td>3.008709</td>\n      <td>1.584090</td>\n      <td>-10.950861</td>\n      <td>7158.408216</td>\n      <td>0.000863</td>\n      <td>98.523098</td>\n      <td>-0.010367</td>\n      <td>0.249339</td>\n      <td>-0.049027</td>\n      <td>-0.001410</td>\n      <td>-0.999242</td>\n      <td>0.049649</td>\n      <td>-0.999415</td>\n      <td>-0.023803</td>\n      <td>-0.247384</td>\n      <td>0.035573</td>\n      <td>0.188556</td>\n      <td>0.055165</td>\n      <td>-0.396573</td>\n      <td>-0.054989</td>\n      <td>-0.190535</td>\n      <td>UNKNOWN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>16.59</td>\n      <td>16.59</td>\n      <td>18</td>\n      <td>18</td>\n      <td>100.0</td>\n      <td>1.689</td>\n      <td>NaN</td>\n      <td>1.649750</td>\n      <td>1.871932</td>\n      <td>-7.274009</td>\n      <td>7168.393928</td>\n      <td>0.001295</td>\n      <td>69.718480</td>\n      <td>-0.058769</td>\n      <td>0.545509</td>\n      <td>-0.108184</td>\n      <td>0.008024</td>\n      <td>-0.997869</td>\n      <td>0.082388</td>\n      <td>-0.999982</td>\n      <td>0.060950</td>\n      <td>-0.547850</td>\n      <td>-0.010167</td>\n      <td>0.728109</td>\n      <td>-0.105330</td>\n      <td>0.935180</td>\n      <td>0.060764</td>\n      <td>-0.730231</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.448043</td>\n      <td>774.094390</td>\n      <td>799.540589</td>\n      <td>780.973267</td>\n      <td>63.901459</td>\n      <td>-16.009902</td>\n      <td>-0.057504</td>\n      <td>205.770885</td>\n      <td>13.384397</td>\n      <td>30.401822</td>\n      <td>1.893778</td>\n      <td>3.925567</td>\n      <td>4.334709</td>\n      <td>6.972808</td>\n      <td>0.497011</td>\n      <td>4.485034</td>\n      <td>-2.532170</td>\n      <td>0.104430</td>\n      <td>-4.974756</td>\n      <td>-2.940259</td>\n      <td>-5.632836</td>\n      <td>-1.638699</td>\n      <td>69</td>\n      <td>77</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>4.055292</td>\n      <td>2</td>\n      <td>-30.000000</td>\n      <td>-6.270511</td>\n      <td>404.887321</td>\n      <td>18789.0</td>\n      <td>14347.0</td>\n      <td>-700.7</td>\n      <td>-5177.3</td>\n      <td>18048.3</td>\n      <td>14.4</td>\n      <td>-13791.4</td>\n      <td>-3957.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.63</td>\n      <td>3.63</td>\n      <td>489</td>\n      <td>488</td>\n      <td>99.1</td>\n      <td>1.104</td>\n      <td>3.4479</td>\n      <td>2.691865</td>\n      <td>1.486174</td>\n      <td>-11.060869</td>\n      <td>7158.407999</td>\n      <td>0.000863</td>\n      <td>98.523106</td>\n      <td>-0.155322</td>\n      <td>0.217544</td>\n      <td>-0.016202</td>\n      <td>0.148930</td>\n      <td>-0.999550</td>\n      <td>0.018769</td>\n      <td>-0.998696</td>\n      <td>0.104859</td>\n      <td>-0.217809</td>\n      <td>-0.098427</td>\n      <td>0.164164</td>\n      <td>0.027754</td>\n      <td>-0.407321</td>\n      <td>-0.028021</td>\n      <td>-0.166710</td>\n      <td>UNKNOWN</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>16.59</td>\n      <td>16.59</td>\n      <td>18</td>\n      <td>18</td>\n      <td>100.0</td>\n      <td>1.689</td>\n      <td>NaN</td>\n      <td>1.649750</td>\n      <td>1.871932</td>\n      <td>-7.274009</td>\n      <td>7168.395399</td>\n      <td>0.001295</td>\n      <td>69.718480</td>\n      <td>-0.159724</td>\n      <td>0.540500</td>\n      <td>-0.046468</td>\n      <td>0.141367</td>\n      <td>-0.999715</td>\n      <td>0.037042</td>\n      <td>-0.999366</td>\n      <td>0.128234</td>\n      <td>-0.545087</td>\n      <td>-0.109780</td>\n      <td>0.722872</td>\n      <td>-0.057421</td>\n      <td>0.935103</td>\n      <td>0.041131</td>\n      <td>-0.727637</td>\n      <td>12.0</td>\n      <td>2.0</td>\n      <td>786.446057</td>\n      <td>774.095941</td>\n      <td>799.542098</td>\n      <td>780.974700</td>\n      <td>63.903569</td>\n      <td>-16.009902</td>\n      <td>-0.057504</td>\n      <td>193.536714</td>\n      <td>13.459160</td>\n      <td>32.427031</td>\n      <td>1.805864</td>\n      <td>3.936785</td>\n      <td>4.542696</td>\n      <td>7.978575</td>\n      <td>0.417638</td>\n      <td>4.484984</td>\n      <td>-2.326744</td>\n      <td>1.109949</td>\n      <td>-5.068911</td>\n      <td>-2.933680</td>\n      <td>-5.664379</td>\n      <td>-1.638476</td>\n      <td>69</td>\n      <td>77</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   event_id  time_to_tca mission_id       risk  max_risk_estimate  \\\n9         2     6.983474          2 -10.816161          -6.601713   \n10        2     6.691611          2 -10.850473          -6.603452   \n11        2     6.269979          2 -30.000000          -6.217958   \n12        2     6.042352          2 -30.000000          -6.271078   \n13        2     5.711716          2 -30.000000          -6.277448   \n14        2     5.377642          2 -30.000000          -6.278272   \n15        2     5.028915          2 -30.000000          -6.283246   \n16        2     4.724355          2 -30.000000          -6.283579   \n17        2     4.334354          2 -30.000000          -6.524619   \n18        2     4.055292          2 -30.000000          -6.270511   \n\n    max_risk_scaling  miss_distance  relative_speed  relative_position_r  \\\n9          13.293159        22902.0         14348.0              -1157.6   \n10         13.374242        22966.0         14348.0              -1161.1   \n11        426.808532        18785.0         14347.0               -698.8   \n12        181.496778        18842.0         14347.0               -700.0   \n13        187.525360        19015.0         14347.0               -709.9   \n14        190.090568        19137.0         14347.0               -710.3   \n15        188.270059        18918.0         14347.0               -714.8   \n16        192.100563        19152.0         14347.0               -717.5   \n17       1984.965171        18635.0         14347.0               -704.3   \n18        404.887321        18789.0         14347.0               -700.7   \n\n    relative_position_t  relative_position_n  relative_velocity_r  \\\n9               -6306.2              21986.3                 15.8   \n10              -6330.2              22046.3                 15.8   \n11              -5176.4              18044.8                 14.4   \n12              -5192.1              18099.4                 14.4   \n13              -5242.1              18264.8                 14.5   \n14              -5273.6              18382.8                 14.5   \n15              -5213.9              18172.0                 14.5   \n16              -5277.4              18397.2                 14.5   \n17              -5134.8              17900.0                 14.4   \n18              -5177.3              18048.3                 14.4   \n\n    relative_velocity_t  relative_velocity_n t_time_lastob_start  \\\n9              -13792.0              -3957.1                 1.0   \n10             -13792.0              -3957.1                 1.0   \n11             -13791.4              -3957.2                 1.0   \n12             -13791.4              -3957.2                 1.0   \n13             -13791.4              -3957.2                 1.0   \n14             -13791.4              -3957.2                 1.0   \n15             -13791.4              -3957.2                 1.0   \n16             -13791.4              -3957.2                 1.0   \n17             -13791.4              -3957.2                 1.0   \n18             -13791.4              -3957.2                 1.0   \n\n   t_time_lastob_end  t_recommended_od_span  t_actual_od_span  \\\n9                0.0                   3.92              3.92   \n10               0.0                   3.86              3.86   \n11               0.0                   3.85              3.85   \n12               0.0                   3.83              3.83   \n13               0.0                   3.72              3.72   \n14               0.0                   3.69              3.69   \n15               0.0                   3.63              3.63   \n16               0.0                   3.72              3.72   \n17               0.0                   3.70              3.70   \n18               0.0                   3.63              3.63   \n\n    t_obs_available  t_obs_used  t_residuals_accepted  t_weighted_rms  \\\n9               444         442                  99.4           1.094   \n10              444         442                  99.4           1.099   \n11              447         445                  99.4           1.113   \n12              451         449                  99.4           1.122   \n13              466         464                  99.5           1.143   \n14              478         475                  99.5           1.119   \n15              478         475                  99.5           1.129   \n16              469         466                  99.4           1.159   \n17              478         476                  99.3           1.103   \n18              489         488                  99.1           1.104   \n\n    t_rcs_estimate  t_cd_area_over_mass  t_cr_area_over_mass     t_sedr  \\\n9           3.4505             3.042086             0.924980 -10.894082   \n10          3.4505             2.880922             1.065057 -10.960651   \n11          3.4505             2.746222             0.965072 -11.022029   \n12          3.4479             2.662064             0.996299 -11.058486   \n13          3.4479             2.808804             1.381067 -11.012908   \n14          3.4479             2.816552             1.414954 -11.019932   \n15          3.4479             3.022060             1.570693 -10.953841   \n16          3.4479             3.398029             1.775686 -10.828875   \n17          3.4479             3.008709             1.584090 -10.950861   \n18          3.4479             2.691865             1.486174 -11.060869   \n\n      t_j2k_sma  t_j2k_ecc  t_j2k_inc    t_ct_r    t_cn_r    t_cn_t  \\\n9   7158.394530   0.000860  98.523094 -0.099768  0.357995 -0.122174   \n10  7158.394561   0.000861  98.523097 -0.005874  0.360471 -0.036075   \n11  7158.407962   0.000862  98.523100 -0.222621  0.425875 -0.149746   \n12  7158.407846   0.000862  98.523108 -0.230120  0.236754 -0.045980   \n13  7158.407228   0.000863  98.523107  0.173348  0.318120  0.169947   \n14  7158.406819   0.000862  98.523103 -0.115261  0.255582  0.007605   \n15  7158.407343   0.000862  98.523115 -0.188946  0.174655 -0.026595   \n16  7158.406303   0.000863  98.523117  0.138205  0.220052  0.043062   \n17  7158.408216   0.000863  98.523098 -0.010367  0.249339 -0.049027   \n18  7158.407999   0.000863  98.523106 -0.155322  0.217544 -0.016202   \n\n    t_crdot_r  t_crdot_t  t_crdot_n  t_ctdot_r  t_ctdot_t  t_ctdot_n  \\\n9    0.085472  -0.999674   0.121504  -0.999114   0.057809  -0.353866   \n10  -0.002789  -0.999876   0.035870  -0.997255  -0.068114  -0.357012   \n11   0.206756  -0.999517   0.147289  -0.999479   0.191052  -0.423717   \n12   0.222933  -0.999848   0.047067  -0.998058   0.169099  -0.236669   \n13  -0.187696  -0.999456  -0.170031  -0.999407  -0.207145  -0.321772   \n14   0.107766  -0.999832  -0.008410  -0.998007   0.052449  -0.257121   \n15   0.182648  -0.999807   0.027637  -0.998078   0.127830  -0.174536   \n16  -0.146262  -0.999742  -0.042066  -0.998311  -0.195389  -0.220230   \n17  -0.001410  -0.999242   0.049649  -0.999415  -0.023803  -0.247384   \n18   0.148930  -0.999550   0.018769  -0.998696   0.104859  -0.217809   \n\n    t_ctdot_rdot  t_cndot_r  t_cndot_t  t_cndot_n  t_cndot_rdot  t_cndot_tdot  \\\n9      -0.043471  -0.025138   0.087954  -0.430583     -0.088821      0.021409   \n10      0.076754  -0.027154   0.084268  -0.442266     -0.085037      0.020991   \n11     -0.175085   0.082662   0.017007  -0.405439     -0.018617     -0.083820   \n12     -0.161825   0.104967   0.002330  -0.433469     -0.003761     -0.106536   \n13      0.221392   0.210865   0.006796  -0.381647     -0.010702     -0.209821   \n14     -0.044921   0.288501  -0.023265  -0.359223      0.022849     -0.288807   \n15     -0.121472   0.286378  -0.055995  -0.380081      0.054933     -0.286015   \n16      0.203368   0.274975   0.056101  -0.395093     -0.058309     -0.275805   \n17      0.035573   0.188556   0.055165  -0.396573     -0.054989     -0.190535   \n18     -0.098427   0.164164   0.027754  -0.407321     -0.028021     -0.166710   \n\n   c_object_type c_time_lastob_start c_time_lastob_end  c_recommended_od_span  \\\n9        UNKNOWN               180.0               2.0                  13.87   \n10       UNKNOWN               180.0               2.0                  13.87   \n11       UNKNOWN                 1.0               0.0                  14.63   \n12       UNKNOWN                 1.0               0.0                  14.63   \n13       UNKNOWN                 1.0               0.0                  14.63   \n14       UNKNOWN                 2.0               1.0                  14.63   \n15       UNKNOWN                 2.0               1.0                  14.63   \n16       UNKNOWN                 2.0               1.0                  14.63   \n17       UNKNOWN                 1.0               0.0                  16.59   \n18       UNKNOWN                 1.0               0.0                  16.59   \n\n    c_actual_od_span  c_obs_available  c_obs_used  c_residuals_accepted  \\\n9              13.87               15          15                 100.0   \n10             13.87               15          15                 100.0   \n11             14.63               15          15                 100.0   \n12             14.63               15          15                 100.0   \n13             14.63               15          15                 100.0   \n14             14.63               15          15                 100.0   \n15             14.63               15          15                 100.0   \n16             14.63               15          15                 100.0   \n17             16.59               18          18                 100.0   \n18             16.59               18          18                 100.0   \n\n    c_weighted_rms  c_rcs_estimate  c_cd_area_over_mass  c_cr_area_over_mass  \\\n9            1.838             NaN             1.579769             2.227246   \n10           1.838             NaN             1.579769             2.227246   \n11           1.641             NaN             1.649021             1.879016   \n12           1.641             NaN             1.649021             1.879016   \n13           1.641             NaN             1.649021             1.879016   \n14           1.641             NaN             1.649021             1.879016   \n15           1.641             NaN             1.649021             1.879016   \n16           1.641             NaN             1.649021             1.879016   \n17           1.689             NaN             1.649750             1.871932   \n18           1.689             NaN             1.649750             1.871932   \n\n      c_sedr    c_j2k_sma  c_j2k_ecc  c_j2k_inc    c_ct_r    c_cn_r    c_cn_t  \\\n9  -7.228422  7168.396928   0.001367  69.717278 -0.068526  0.636970 -0.038214   \n10 -7.228422  7168.397641   0.001367  69.717278 -0.067750  0.636974 -0.038143   \n11 -7.247312  7168.395887   0.001297  69.718437  0.025977  0.563595  0.065183   \n12 -7.247312  7168.396232   0.001297  69.718436 -0.199922  0.552272  0.010836   \n13 -7.247312  7168.396846   0.001297  69.718436 -0.194648  0.552833  0.011237   \n14 -7.247312  7168.397203   0.001297  69.718436 -0.193936  0.552834  0.011532   \n15 -7.247312  7168.395801   0.001297  69.718436 -0.198442  0.552433  0.010928   \n16 -7.247312  7168.396820   0.001297  69.718436 -0.192786  0.553006  0.011435   \n17 -7.274009  7168.393928   0.001295  69.718480 -0.058769  0.545509 -0.108184   \n18 -7.274009  7168.395399   0.001295  69.718480 -0.159724  0.540500 -0.046468   \n\n    c_crdot_r  c_crdot_t  c_crdot_n  c_ctdot_r  c_ctdot_t  c_ctdot_n  \\\n9    0.064305  -0.999989   0.036762  -0.996314   0.153806  -0.634961   \n10   0.063521  -0.999989   0.036689  -0.996313   0.153053  -0.634998   \n11  -0.045196  -0.999602  -0.075887  -0.999774  -0.006036  -0.564147   \n12   0.192984  -0.999944  -0.014799  -0.999656   0.192663  -0.554558   \n13   0.187592  -0.999942  -0.015262  -0.999649   0.187206  -0.555115   \n14   0.186793  -0.999941  -0.015606  -0.999640   0.186100  -0.555175   \n15   0.191476  -0.999943  -0.014906  -0.999639   0.190277  -0.554820   \n16   0.185672  -0.999941  -0.015490  -0.999640   0.184898  -0.555334   \n17   0.008024  -0.997869   0.082388  -0.999982   0.060950  -0.547850   \n18   0.141367  -0.999715   0.037042  -0.999366   0.128234  -0.545087   \n\n    c_ctdot_rdot  c_cndot_r  c_cndot_t  c_cndot_n  c_cndot_rdot  c_cndot_tdot  \\\n9      -0.149627   0.715984  -0.159057   0.953945      0.156803     -0.723349   \n10     -0.148865   0.715914  -0.158753   0.953971      0.156495     -0.723302   \n11      0.025308   0.703561  -0.027022   0.916588      0.007301     -0.706289   \n12     -0.185664   0.694842  -0.051859   0.916218      0.044474     -0.699265   \n13     -0.180088   0.695361  -0.051522   0.916218      0.044023     -0.699780   \n14     -0.178894   0.695308  -0.051279   0.916219      0.043688     -0.699793   \n15     -0.183248   0.695002  -0.051780   0.916216      0.044368     -0.699527   \n16     -0.177721   0.695521  -0.051359   0.916215      0.043803     -0.699987   \n17     -0.010167   0.728109  -0.105330   0.935180      0.060764     -0.730231   \n18     -0.109780   0.722872  -0.057421   0.935103      0.041131     -0.727637   \n\n    t_span  c_span     t_h_apo     t_h_per     c_h_apo     c_h_per  \\\n9     12.0     2.0  786.417082  774.097978  800.056782  780.463075   \n10    12.0     2.0  786.420510  774.094612  800.057080  780.464203   \n11    12.0     2.0  786.439755  774.102169  799.554662  780.963112   \n12    12.0     2.0  786.439933  774.101759  799.555083  780.963381   \n13    12.0     2.0  786.444957  774.095498  799.555077  780.964615   \n14    12.0     2.0  786.443328  774.096310  799.555148  780.965258   \n15    12.0     2.0  786.444058  774.096628  799.553665  780.963937   \n16    12.0     2.0  786.443569  774.095036  799.554287  780.965353   \n17    12.0     2.0  786.448043  774.094390  799.540589  780.973267   \n18    12.0     2.0  786.446057  774.095941  799.542098  780.974700   \n\n    geocentric_latitude    azimuth  elevation  mahalanobis_distance  \\\n9             63.955771 -16.008858  -0.063092            115.208802   \n10            63.956674 -16.008858  -0.063092            101.429474   \n11            63.903391 -16.009902  -0.057504            177.272242   \n12            63.904169 -16.009902  -0.057504            134.494670   \n13            63.906532 -16.009902  -0.057903            194.741883   \n14            63.908209 -16.009902  -0.057903            149.531226   \n15            63.905222 -16.009902  -0.057903            157.237923   \n16            63.908418 -16.009902  -0.057903            169.699821   \n17            63.901459 -16.009902  -0.057504            205.770885   \n18            63.903569 -16.009902  -0.057504            193.536714   \n\n    t_position_covariance_det  c_position_covariance_det  t_sigma_r  \\\n9                   15.229084                  42.445608   2.201549   \n10                  16.265328                  42.441549   2.196657   \n11                  15.145344                  31.967553   2.295355   \n12                  15.944390                  33.940263   2.213602   \n13                  14.475070                  33.909269   2.109108   \n14                  15.588817                  33.884905   2.122911   \n15                  15.091503                  33.932893   2.043324   \n16                  15.011373                  33.893980   2.041773   \n17                  13.384397                  30.401822   1.893778   \n18                  13.459160                  32.427031   1.805864   \n\n    c_sigma_r  t_sigma_t  c_sigma_t  t_sigma_n  c_sigma_n  t_sigma_rdot  \\\n9    5.549886   4.994608  10.549895   0.496310   5.385613     -1.875151   \n10   5.549796   5.490139  10.547926   0.516145   5.385589     -1.378157   \n11   3.879431   4.803485   7.832651   0.601252   4.465008     -2.064897   \n12   3.905626   5.306960   8.812877   0.507719   4.465087     -1.562397   \n13   3.904538   4.663119   8.797461   0.541527   4.465003     -2.204913   \n14   3.904485   5.175424   8.785229   0.537283   4.464941     -1.692445   \n15   3.905318   5.066500   8.809227   0.469616   4.465049     -1.802808   \n16   3.904195   4.976370   8.789843   0.522089   4.464934     -1.891187   \n17   3.925567   4.334709   6.972808   0.497011   4.485034     -2.532170   \n18   3.936785   4.542696   7.978575   0.417638   4.484984     -2.326744   \n\n    c_sigma_rdot  t_sigma_tdot  c_sigma_tdot  t_sigma_ndot  c_sigma_ndot  F10  \\\n9       3.681239     -4.670266     -1.309462     -5.550399     -1.080559   73   \n10      3.679266     -4.669890     -1.309606     -5.536811     -1.080597   73   \n11      0.961933     -4.579886     -2.987280     -5.530733     -1.644409   71   \n12      1.943879     -4.666855     -2.964674     -5.541783     -1.643388   71   \n13      1.928403     -4.752686     -2.965744     -5.515744     -1.643389   71   \n14      1.916182     -4.750897     -2.965900     -5.584831     -1.643389   70   \n15      1.940231     -4.834537     -2.965160     -5.611228     -1.643385   70   \n16      1.920781     -4.817995     -2.966167     -5.596369     -1.643383   70   \n17      0.104430     -4.974756     -2.940259     -5.632836     -1.638699   69   \n18      1.109949     -5.068911     -2.933680     -5.664379     -1.638476   69   \n\n    F3M  SSN  AP  \n9    77   27   4  \n10   77   27   4  \n11   77   23   8  \n12   77   23   8  \n13   77   23   8  \n14   77   11   5  \n15   77   11   5  \n16   77   11   5  \n17   77    0   5  \n18   77    0   5  "},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 151099 entries, 9 to 162633\nColumns: 103 entries, event_id to AP\ndtypes: category(7), float64(88), int16(8)\nmemory usage: 106.7 MB\n"}]},{"metadata":{},"cell_type":"markdown","source":"## Time-Series Forecasting problem"},{"metadata":{},"cell_type":"markdown","source":"### Converting data from Pandas DataFrame to Pytorch Tensors"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tqdm import tqdm\nfrom tqdm import trange\n\n# Get input variable features from config file.\nin_var_features = list(config.get_features(**{'input':True, 'variable':True}).keys())\nin_features     = list(config.get_features(**{'input':True}).keys())\n\n# Get time-series sets for every continuous variable feature \n# (constant features by definition do not need to be forecasted)\ntensor_filename = f'training_tsf_ws{window_size}-f{events_to_forecast}.pt'\n\n# Check if file containing tensors is available in the data folder and load it\nfilepath         = os.path.join(cwd,'data','tensors', tensor_filename)\nfeatures_tensors = torch.load(filepath) if os.path.exists(filepath) else {}\n\nin_var_features = ['t_cr_area_over_mass', 't_sedr', 't_ct_r', 't_cn_r',\n                      't_cn_t', 't_crdot_r', 't_crdot_t', 't_crdot_n']\n\n# Get all input features from which the tensors have not been extracted yet.\nremaining_features = [f for f in in_var_features if not f in list(features_tensors.keys())]\n\n\nprint(f'Features already available in tensor file: {len(list(features_tensors.keys()))}')\n\n# Iterate over all remaining features to get the time series subsets\nt = trange(len(remaining_features), desc='Extracting sequences of time-series ...', leave=True)\n\nfor f in t:\n\n    # Initialize list of tensors for feature f\n    feature = remaining_features[f]\n    features_tensors[feature] = []\n\n    for e, event_id in enumerate(events_filter):\n\n        # Update progress bar\n        tqdm_desc = f'Extracting sequences of time-series from {feature}' \n                    f' {\".\"*(30-len(feature))} (Progress: {(e+1)/len(events_filter)*100:5.1f}%)'\n        t.set_description(tqdm_desc)\n        t.refresh()\n\n        # Get full sequence from dataset and convert it to a tensor.\n        feature_dtype = str(df[df['event_id']==event_id][feature].dtype).lower()\n        full_seq = df[df['event_id']==event_id][feature].to_numpy(dtype=feature_dtype)\n        full_seq = torch.nan_to_num(torch.FloatTensor(full_seq))\n\n        # Add Time-Series subsets from full sequence tensor and add it to the list for the feature f\n        features_tensors[feature] = features_tensors[feature] + rnn.event_ts_sets(full_seq, window_size)\n\n    if (f+1)%10==0 or (f+1==len(remaining_features)):\n        # Save tensors containing all Time-Series subsets for training organised by feature.\n        t.set_description(f'Saving tensors with sequences of time-series into external file {\".\"*(len(tqdm_desc)-64)}')\n        t.refresh()\n        torch.save(features_tensors, filepath)","execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"Features already available in tensor file: 8\n"},{"name":"stderr","output_type":"stream","text":"Extracting sequences of time-series ...: 0it [00:00, ?it/s]\n"}]},{"metadata":{},"cell_type":"markdown","source":"#### Embedding categorical input features\n\nAn embedding is a vector representation of a categorical variable. The representation of this vector is computed through the use of NN models/techniques that take into account potential relation between categories in order to create the vector representation for each category.\n\nIn practice, an embedding matrix is a lookup table for a vector. Each row of an embedding matrix is a vector for a unique category.\n\nThe main advantadge of using embeddings instead of One Hot/Dummy Encoding techniques (one column per unique value of categorical feature with 0s and 1s) is that it can preserve the natural order and common relationships between the categorical features. For example, we could represent the days of the week with 4 floating-point numbers each, and two consecutive days would look more similar than two weekdays that are days apart from each other.\n\n\nThe rule of thumb for determining the embedding size (number of elemens per array) is to divide the number of unique entries in each column by 2, but not to exceed 50."},{"metadata":{"trusted":false},"cell_type":"code","source":"class EventPropagation(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100, out_size=1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        \n        # Add an LSTM layer:\n        self.lstm = nn.LSTM(input_size,hidden_size)\n        \n        # Add a fully-connected layer:\n        self.linear = nn.Linear(hidden_size,out_size)\n        \n        # Initialize h0 and c0:\n        self.hidden = (torch.zeros(1,1,hidden_size),\n                       torch.zeros(1,1,hidden_size))\n    \n    def forward(self,seq):\n        # Get output from LSTM layer and the h0 c0 values updated (passed through LSTM)\n        lstm_out, self.hidden = self.lstm(seq.view(len(seq), 1, -1), self.hidden)\n\n        # Predict next values with the Linear layer\n        pred = self.linear(lstm_out.view(len(seq),-1))\n\n        # Return only last value\n        return pred[-1]\n\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Define Multivariate LSTM network class\nclass EventPropagation(nn.Module):\n    def __init__(self,input_size, hidden_size, output_size, seq_length, num_layers=1):\n        super(EventPropagation, self).__init__()\n        self.input_size = input_size    # Number of input features\n        self.hidden_size = hidden_size  # Number of hidden neurons\n        self.output_size = output_size  # Number of outputs\n        self.num_layers = num_layers    # Number of recurrent (stacked) layers\n        self.seq_length = seq_length\n    \n        self.lstm = nn.LSTM(input_size = self.input_size, \n                            hidden_size = self.hidden_size,\n                            num_layers = self.num_layers,\n                            batch_first = True)\n        # according to pytorch docs LSTM output is \n        # (batch_size,seq_len, num_directions * hidden_size)\n        # when considering batch_first = True\n        self.linear = nn.Linear(self.hidden_size*self.seq_length, \n                                self.output_size)\n        \n    \n    def init_hidden(self, n_sequences):\n        # Initialize states. Even with batch_first = True this remains same as docs\n        h_state = torch.zeros(self.num_layers, n_sequences, self.hidden_size) # Hidden state\n        c_state = torch.zeros(self.num_layers, n_sequences, self.hidden_size) # Cell state\n        self.hidden = (h_state, c_state)\n    \n    \n    def forward(self, inputs):        \n        n_sequences, seq_length, n_features = inputs.size()\n        \n        lstm_out, self.hidden = self.lstm(inputs, self.hidden)\n        # lstm_out(with batch_first = True) is \n        # (batch_size,seq_len,num_directions * hidden_size)\n        # for following linear layer we want to keep batch_size dimension and merge rest       \n        # .contiguous() -> solves tensor compatibility error\n        inputs = lstm_out.contiguous().view(n_sequences,-1)\n        outputs = self.linear(x)\n        \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of features and number of sequences to process from tensor file\nfeatures    = list(features_tensors.keys())\nn_sequences = len(features_tensors[features[0]])\nseq_length  = 5 # Window size for the TSF\n\n# Initialize inputs and outputs arrays to contain sequences to process\ninputs = np.full(shape = (n_sequences,len(features)), fill_value = None)\noutputs = np.full(shape = (n_sequences,len(features)), fill_value = None)\n\n# Initialize trange object for sequences to print progress bar.\nsequences = trange(n_sequences, desc='Getting training and target tensors ...', leave=True)\nfor s in sequences:\n\n    # Initialize list for sequence s\n    inputs_s    = []\n    outputs_s   = []\n    # Get sequence s from all features\n    for f, feature in enumerate(features):\n        \n        # Update progress bar\n        tqdm_desc = f'Processing sequence {s:<6} from all features' \n                    f' (Progress: {(f+1)/len(features)*100:5.1f}%)'\n        t.set_description(tqdm_desc)\n        t.refresh()\n        \n        # Get sequence s (input and output) from feature f\n        # - inputs_sf  = [f1_t1, f1_t2, ..., f1_tn]\n        # - outputs_sf = [f1_tn+1]\n        inputs_sf, outputs_sf = features_tensors[feature][s]\n        \n        # Get sequence s (input and output) from feature f\n        # - inputs_s  = [[f1_t1, f1_t2, ..., f1_tn], [f2_t1, f2_t2, ..., f2_tn], ...]\n        # - outputs_s = [[f1_tn+1], [f2_tn+1], ...]\n        inputs_s.append(inputs_sf)\n        outputs_s.append(outputs_sf)\n        \n    # Get sequence s (input and output) from feature f\n    # - inputs  = [[f1_t1, f2_t1, ..., fn_t1], [f1_t2, f2_t2, ..., fn_t2], ...]\n    # - outputs = [[f1_tn+1, f2_tn+1, ...], ...]\n    inputs[s]  = np.transpose(np.asanyarray(inputs_s, dtype='float32'))\n    outputs[s] = np.transpose(np.asarray(outputs_s, dtype='float32').flatten())\n\n# Convert numpy array to tensors\ninputs  = torch.tensor(inputs, dtype=torch.float32)\noutputs = torch.tensor(outputs, dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instanciate model with required inputs.\ntorch.manual_seed(42)\nmodel = EventPropagation(input_size = n_features, \n                         hidden_size = 100,\n                         output_size = n_features,\n                         seq_length = seq_length)\n\n# Define criterion and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n\n# Print model\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()\n\n# model.train()\n\nepochs = 500\nbatch_size = 16\n\n# Initialize array of inf values for losses\nlosses = np.ones(epochs)*np.inf\n\n# Iterate over all remaining features to get the time series subsets\nt = trange(epochs, desc='Extracting sequences of time-series ...', leave=True)\n\nfor e in t:\n    \n    # Train model by passing n batches depending on the batch_size\n    for b in range(0, n_sequences, batch_size):\n        \n        # Get inputs and outputs for batch b\n        inputs_b  = inputs[b:b+batch_size, :, :]\n        outputs_b = outputs[b:b+batch_size]  \n        \n        # Reset Gradient from the optimizer (hidden and cell states)\n        optimizer.zero_grad()\n        \n        # Initialize hidden state and compute outputs\n        model.init_hidden(inputs_b.size(0))\n        forecast = model(inputs_b) \n        \n        # Compute loss using the outputs for the batch b and store values in array\n        loss = criterion(forecast[-1].view(-1), outputs_b)  \n        losses[e] = loss.detach().numpy()\n        \n        # Back propagate loss and adjust parameters of the optimizer\n        loss.backward()\n        optimizer.step()\n        \n        # Update progress bar\n        t.set_description(f'Training Conjunction Event Propagation model | MSE loss = {loss.item():10.8f} ')\n        t.refresh()\n        \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Save the trained model to a file\nRight now <strong><tt>model</tt></strong> has been trained and validated, and seems to correctly classify an iris 97% of the time. Let's save this to disk.<br>\nThe tools we'll use are <a href='https://pytorch.org/docs/stable/torch.html#torch.save'><strong><tt>torch.save()</tt></strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.load'><strong><tt>torch.load()</tt></strong></a><br>\n\nThere are two basic ways to save a model.<br>\n\nThe first saves/loads the `state_dict` (learned parameters) of the model, but not the model class. The syntax follows:<br>\n<tt><strong>Save:</strong>&nbsp;torch.save(model.state_dict(), PATH)<br><br>\n<strong>Load:</strong>&nbsp;model = TheModelClass(\\*args, \\*\\*kwargs)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.load_state_dict(torch.load(PATH))<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n\nThe second saves the entire model including its class and parameters as a pickle file. Care must be taken if you want to load this into another notebook to make sure all the target data is brought in properly.<br>\n<tt><strong>Save:</strong>&nbsp;torch.save(model, PATH)<br><br>\n<strong>Load:</strong>&nbsp;model = torch.load(PATH))<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.eval()</tt>\n\nIn either method, you must call <tt>model.eval()</tt> to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n\nFor more information visit https://pytorch.org/tutorials/beginner/saving_loading_models.html"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}