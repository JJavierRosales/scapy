{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ecbe21",
   "metadata": {},
   "source": [
    "# [Collision Avoidance Challenge](https://kelvins.esa.int/collision-avoidance-challenge/home/)\n",
    "## Introduction\n",
    "Today, active **collision avoidance** among orbiting satellites has become a routine task in space operations, relying on validated, accurate and timely space surveillance data. For a typical satellite in Low Earth Orbit, hundreds of alerts are issued every week corresponding to possible close encounters between a satellite and another space object (in the form of conjunction data messages CDMs). After automatic processing and filtering, there remain about 2 actionable alerts per spacecraft and week, requiring detailed follow-up by an analyst. On average, at the European Space Agency, more than one collision avoidance manoeuvre is performed per satellite and year.\n",
    "\n",
    "\n",
    "In this challenge, you are tasked to build a model to predict the final collision risk estimate between a given satellite and a space object (e.g. another satellite, space debris, etc). To do so, you will have access to a database of real-world conjunction data messages (CDMs) carefully prepared at ESA. Learn more about the challenge and the data.\n",
    "\n",
    "This competition is organized by ESA's Advanced Concepts Team (ACT) in partnership with ESA's Space Debris Office\n",
    "\n",
    "Experts from both teams are available for interactions during the competition.\n",
    "\n",
    "## Challenge\n",
    "\n",
    "As of estimations done in January 2019, more than 34,000 objects with a size larger than 10cm are orbiting our planet. Of these, 22,300 are tracked by the Space Surveillance Network and their position released in the form of a globally shared catalogue.\n",
    "\n",
    "\n",
    "ESA's Space Debris Office supports collision avoidance activities covering the ESA missions Aeolus, Cryosat-2 and the constellation of Swarm-A/B/C in low-Earth orbit and Cluster-II in highly eccentric orbit approaching the Geostationary (GEO) region. On top of these, more than a dozen spacecraft of partner agencies and commercial operators are supported.\n",
    "\n",
    "![Alt text](https://kelvins.esa.int/media/public/ckeditor_uploads/2021/08/05/new_swarm.png \"Title\")\n",
    "\n",
    "In the context of this activity, the orbits of these satellites are propagated and when a close approach with any object in the catalogue is detected a **Conjunction Data Message** (CDM) is assembled and released. Each CDM **contains multiple attributes** about the approach, such as the identity of the satellite in question, the object type of the potential collider, the time of closest approach (TCA), the uncertainty (i.e. covariances), etc. It also contains a **self-reported** risk, which is computed using some of the attributes from the CDM. In the days following the first CDM, as the uncertainties of the objects positions become smaller, other CDMs are released refining the knowledge acquired on the close encounter.\n",
    "\n",
    "Typically, a **time series** of CDMs covering one week is released **for each unique close approach**, with about 3 CDMs becoming available per day. For a given close approach the last obtained CDM, including the computed risk, can be assumed to be the **best knowledge** we have about the potential collision and the state of the two objects in question. In most cases, the Space Debris Office will alarm control teams and start thinking about a potential avoidance manoeuvre 2 days prior to the close approach in order to avoid the risk of collision, to then make a final decision 1 day prior. In this challenge, we ask to **build a model** that makes use of the CDMs recorded up to 2 days prior to the closest approach to **predict the final risk** (i.e. the risk predicted in the last available CDM prior to close approach).\n",
    "\n",
    "More about the dataset used in this competition and the attributes contained in the various CDMs can be found in the data section. You can also learn some more about the current way ESA's Space Debris office deals with collision avoidance manoeuvres reading this [paper](https://kelvins.esa.int/media/public/competitions/collision-avoidance-challenge/SDC7-paper1017.pdf).\n",
    "\n",
    "We thank the US Space Surveillance Network for the provision of surveillance data supporting safe operations of ESA’s spacecraft. Specifically, we are grateful to the agreement which allows to publicly release the dataset for the purpose of this competition.\n",
    "\n",
    "## Data\n",
    "\n",
    "### Differences Between Training and Testing Data\n",
    "Each dataset is made of several unique events (close encounters betwen two objects) which are indexed by a unique number in the `event_i` column.\n",
    "\n",
    "- The `training` set has 162634 rows and **13154 unique events** (giving on average about 12 rows/CDMs per close encounter).\n",
    "\n",
    "- The `testing` set has 24484 rows and **2167 unique events** (giving on average about 11 rows/CDMs per close encounter).\n",
    "\n",
    "**Important: Note that the `testing` set and the `training` set have not been randomly sampled from the database. In other words, while they come from the same database, with the same collection process and the same features, they have been hand picked in order to over-represent high risk events and to create an interesting predictive model. This is a characterstic of this competition where high risk events are scarce, but represent the true final target of a useful predictive model.**\n",
    "\n",
    "In particular, the `testing` data differs in two major ways compared to the `training` set:\n",
    "\n",
    " * It only contains events for which the latest CDM is within 1 day ( `time_to_tc` < 1) of the time to closest approach (TCA). This is because, in some cases, the latest available CDM is days away from the (known) time to closest approach. It would be wrong to assume that the computed risk 7 days before the actual time to closest approach can be a good approximation to the risk at TCA. Furthermore, predicting the risk many days prior the time to closest approach is not of great interest to us. On the other hand, the `training` set is unfiltered and you will find many cases where the latest available CDMs is days away from the TCA. We have chosen to keep these collision events in the training set because they may still be useful when it comes to predicting events from the test set.\n",
    "\n",
    " * There are no CDMs to learn from which are within 2 days of the TCA. In other words, the data available closest to the TCA will be at least 2 days away. This is because, as mentioned in the challenge section, a potential avoidance manoeuvre is planned at least 2 days prior to closest approach. Similarly to the above, the `training` set will contain all cases, including events where no data is available at least 2 days prior to closest approach (i.e. events with all their CDMs being within 2 days of TCA are still present in the dataset).\n",
    " \n",
    "### Columns Description\n",
    "\n",
    "The dataset is represented as a table, where each row correspond to a single CDM, and each CDM contains 103 recorded characteristics/features. There are thus 103 columns, which we describe below. The dataset is made of several unique collision/close approach events, which are identified in the `event_id` column. In turn, each collision event is made of several CDMs recorded over time. Therefore, a single collision event can be thought of as a times series of CDMs. From these CDMs, for every collision event, we are interested in predicting the final risk which is computed in the last CDM of the time series (i.e. the risk value in the last row of each collision event).\n",
    "\n",
    "For the column description, we first describe columns which have unique names and then the columns whose name difference only depends on whether they are referring to the target object (if the column name starts with a **t**) or the chaser object (if the column name starts with a **c**). Here, target refers to the ESA satellites while chaser refers to the space debris/object we want to avoid. describe the column names shared for both the chaser and the target, we replace **t** and **c** with the placeholder **x**. For instance, `c_sigma_r` and `t_sigma_r` both correspond to the description of `x_sigma_r`.\n",
    "\n",
    "Note that all the columns are numerical except for `c_object_type`.\n",
    "\n",
    "#### Uniquely Named Columns\n",
    "- `risk`:self-computed value at the epoch of each CDM [base 10 log]. **In the test set, this value is to be predicted, at the time of closest approach for each `event_id`. Note that, as mentioned above, in the `test` set, we do not know the actual data contained in CDMs that are within 2 days to closest approach, since they happen in the \"future\".**\n",
    "- `event_id`: unique id per collision event\n",
    "- `time_to_tca`: Time interval between CDM creation and time-of-closest approach [days]\n",
    "- `mission_id`: identifier of mission that will be affected\n",
    "- `max_risk_estimate`: maximum collision probability obtained by scaling combined covariance\n",
    "- `max_risk_scaling`: scaling factor used to compute maximum collision probability\n",
    "- `miss_distance`: relative position between chaser & target at tca [m]\n",
    "- `relative_speed`: relative speed between chaser & target at tca [m/s]\n",
    "- `relative_position_n`: relative position between chaser & target: normal (cross-track) [m]\n",
    "- `relative_position_r`: relative position between chaser & target: radial [m]\n",
    "- `relative_position_t`: relative position between chaser & target: transverse (along-track) [m]\n",
    "- `relative_velocity_n`: relative velocity between chaser & target: normal (cross-track) [m/s]\n",
    "- `relative_velocity_r`: relative velocity between chaser & target: radial [m/s]\n",
    "- `relative_velocity_t`: relative velocity between chaser & target: transverse (along-track) [m/s]\n",
    "- `c_object_type`: object type which is at collision risk with satellite\n",
    "- `geocentric_latitude`: Latitude of conjunction point [deg]\n",
    "- `azimuth`: relative velocity vector: azimuth angle [deg]\n",
    "- `elevation`: relative velocity vector: elevation angle [deg]\n",
    "- `F10`: 10.7 cm radio flux index [10−2210−22 W/(m2m2 Hz)]\n",
    "- `AP`: daily planetary geomagnetic amplitude index\n",
    "- `F3M`: 81-day running mean of F10.7 (over 3 solar rotations) [10−2210−22 W/(m2m2 Hz)]\n",
    "- `SSN`: Wolf sunspot number\n",
    "\n",
    "#### Shared Column Names Between the Chaser and the Target Object\n",
    " - `x_sigma_rdot`: covariance; radial velocity standard deviation (sigma) [m/s]\n",
    " - `x_sigma_n`: covariance; (cross-track) position standard deviation (sigma) [m]\n",
    " - `x_cn_r`: covariance; correlation of normal (cross-track) position vs radial position\n",
    " - `x_cn_t`: covariance; correlation of normal (cross-track) position vs transverse (along-track) position\n",
    " - `x_cndot_n`: covariance; correlation of normal (cross-track) velocity vs normal (cross-track) position\n",
    " - `x_sigma_ndot`: covariance; normal (cross-track) velocity standard deviation (sigma) [m/s]\n",
    " - `x_cndot_r`: covariance; correlation of normal (cross-track) velocity vs radial position\n",
    " - `x_cndot_rdot`: covariance; correlation of normal (cross-track) velocity vs radial velocity\n",
    " - `x_cndot_t`: covariance; correlation of normal (cross-track) velocity vs transverse (along-track) position\n",
    " - `x_cndot_tdot`: covariance; correlation of normal (cross-track) velocity vs transverse (along-track) velocity\n",
    " - `x_sigma_r`: covariance; radial position standard deviation (sigma) [m]\n",
    " - `x_ct_r`: covariance; correlation of transverse (along-track) position vs radial position\n",
    " - `x_sigma_t`: covariance; transverse (along-track) position standard deviation (sigma) [m]\n",
    " - `x_ctdot_n`: covariance; correlation of transverse (along-track) velocity vs normal (cross-track) position\n",
    " - `x_crdot_n`: covariance; correlation of radial velocity vs normal (cross-track) position\n",
    " - `x_crdot_t`: covariance; correlation of radial velocity vs transverse (along-track) position\n",
    " - `x_crdot_r`: covariance; correlation of radial velocity vs radial position\n",
    " - `x_ctdot_r`: covariance; correlation of transverse (along-track) velocity vs radial position\n",
    " - `x_ctdot_rdot`: covariance; correlation of transverse (along-track) velocity vs radial velocity\n",
    " - `x_ctdot_t`: covariance; correlation of transverse (along-track) velocity vs transverse (along-track) position\n",
    " - `x_sigma_tdot`: covariance; transverse (along-track) velocity standard deviation (sigma) [m/s]\n",
    " - `x_position_covariance_det`: determinant of covariance (~volume)\n",
    " - `x_cd_area_over_mass`: ballistic coefficient [m2m2/kg]\n",
    " - `x_cr_area_over_mass`: solar radiation coefficient . A/m (ballistic coefficient equivalent)\n",
    " - `x_h_apo`: apogee (-RearthRearth) [km]\n",
    " - `x_h_per`: perigee (-RearthRearth)[km]\n",
    " - `x_j2k_ecc`: eccentricity\n",
    " - `x_j2k_inc`: inclination [deg]\n",
    " - `x_j2k_sma`: semi-major axis [km]\n",
    " - `x_sedr`: energy dissipation rate [W/kg]\n",
    " - `x_span`: size used by the collision risk computation algorithm (minimum 2 m diameter assumed for the chaser) [m]\n",
    " - `x_rcs_estimate`: radar cross-sectional area [m2m2]\n",
    " - `x_actual_od_span`: actual length of update interval for orbit determination [days]\n",
    " - `x_obs_available`: number of observations available for orbit determination (per CDM)\n",
    " - `x_obs_used`: number of observations used for orbit determination (per CDM)\n",
    " - `x_recommended_od_span`: recommended length of update interval for orbit determination [days]\n",
    " - `x_residuals_accepted`: orbit determination residuals\n",
    " - `x_time_lastob_end`: end of the time interval in days (with respect to the CDM creation epoch) of the last accepted observation used in the orbit determination\n",
    " - `x_time_lastob_start`: start of the time in days (with respect to the CDM creation epoch) of the last accepted observation used in the orbit determination\n",
    " - `x_weighted_rms`: root-mean-square in least-squares orbit determination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f668c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "The development of any Machine Learning model requires a solid knowledge of the data available for its training and subsequent development in order to guarantee a successful deployment and applicability to real-world problems. For this purpose, in this section we develop a comprehensive *Exploratory Data Analysis* (EDA) on the Kelvins Collision Avoidance Challenge `training` and `test` with two main objectives in mind:\n",
    " - Acquire a solid understanding of real data received in a conventional Conjunction Data Message (CDM); this involves a basic analysis of the data distribution and its potential common patterns for ***Target*** and ***Chaser*** objects, and cluster identification for conjunction events.\n",
    " \n",
    "- Develop a Synthetic Data Generation (SDG) process that can reliably produce additional virtual (non-existing) data with the objective to enrich the Time-Series Forecasting Deep Learning model and improve its performance in production by reinforcing its training process.\n",
    "\n",
    "For this purpose, this notebook is structured in the following sections:\n",
    "\n",
    "1. Data import and initial exploration.\n",
    "2. Data distribution analysis.\n",
    "3. Probability Density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8394609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries required for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Import Scikit-learn required libraries\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Import function to clear output\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import file system libraries\n",
    "from pathlib import Path\n",
    "\n",
    "# Import matplotlib library and setup environment for plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "from matplotlib import rc, gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors \n",
    "\n",
    "# Set rendering parameters to use TeX font.\n",
    "if not '/private/var/' in os.getcwd():\n",
    "    rc('font', **{'family': 'serif', 'serif': ['Computer Modern'], 'size': 11})\n",
    "    rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb3baa",
   "metadata": {},
   "source": [
    "## 1. - Data import and initial exploration\n",
    "\n",
    "In this section, we import and explore the data provided in the competition with the aim to identify all those fields relevant for the synthetic data generation process and the Time-Series Forecasting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory path for the tool parent folder and print it.\n",
    "parent_folder = 'Tool'\n",
    "cwd = str(Path(os.getcwd()[:os.getcwd().index(parent_folder)+len(parent_folder)]))\n",
    "print('Parent working directory: \\n%s' % cwd)\n",
    "\n",
    "# Set maximum number of columns to show as None\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Import training dataset\n",
    "df = pd.read_csv(os.path.join(cwd,'data','esa-challenge','train_data.csv'), \n",
    "                 sep=',', header=0, index_col=None, skipinitialspace=False)\n",
    "\n",
    "# Sort values of dataframe by event_id and time_to_tca and re-index\n",
    "df.sort_values(by=['event_id', 'time_to_tca'], axis='index', \n",
    "               ascending=[True,False], inplace=True, ignore_index=True)\n",
    "\n",
    "# Show first n rows of dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d93be2",
   "metadata": {},
   "source": [
    "\n",
    "As described in the introduction of Kelvins competition, both `training` and `test` datasets contain several CDMs per `event_id`, being the **last CDM** for a given `event_id` the one with the **most accurate collision probability risk estimation**. Following the same assumption for the keplerian elements measurements, a subset `df_lastCDM` from the origin datasets is created containing only the last CDM. \n",
    "\n",
    "This subset will be used in subsequent sections of the EDA to understand how population of ASOs is distributed across its different dimensions so that synthetic data can be generated from that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only last CDM data from every event_id\n",
    "df_lastCDM = df.drop_duplicates('event_id', keep='last')\n",
    "\n",
    "# Show first n rows of dataframe with only final CDMs.\n",
    "df_lastCDM.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0e28b",
   "metadata": {},
   "source": [
    "## 2. - Data distribution analysis\n",
    "\n",
    "In order to generate synthetic data that reliably describe the current problem of ASOs population in the near Earth environment it is crucial to analyse their keplerian orbital elements. This analysis must be performed on the two types of objects involved in the conjunction events (*Target* and *Chaser*), and the orbital elements data to be considered the following:\n",
    "\n",
    " - `x_h_apo` (Apogee altitude)\n",
    " - `x_h_per` (Perigee)\n",
    " - `x_j2k_ecc` (Eccentricity)\n",
    " - `x_j2k_inc` (Inclination)\n",
    " - `x_j2k_sma` (Semi-major axis)\n",
    "\n",
    "In addition to the orbital elements, the analysis of the `miss_distance` data distribution is also of crucial interest as it constitutes one of the starting points in the synthetic data generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to define upper and lower limits for standard data in a population\n",
    "def outliers_boundaries(data, threshold = 2.0):\n",
    "\n",
    "    Q1 = np.quantile(data,0.25)\n",
    "    Q3 = np.quantile(data,0.75)\n",
    "    IQR = st.iqr(data)\n",
    "    \n",
    "    return (max(0, (Q1-IQR*threshold)), (Q3+IQR*threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to estimate adequate number of bins for histograms.\n",
    "def nbins(data, rule):\n",
    "    \n",
    "    # Get the number of items within the dataset\n",
    "    n = len(data)\n",
    "    \n",
    "    # Compute the histogram bins size (range)\n",
    "    bins_size = {'sturge': 1 + 3.322*np.log(n),    \n",
    "                 'scott': 3.49*np.std(data)*n**(-1/3),                       \n",
    "                 'rice': 2*n**(1/3),                         \n",
    "                 'fd': 2*st.iqr(data)*n**(-1/3)}\n",
    "    \n",
    "    # Return number of bins\n",
    "    return math.ceil((data.max() - data.min())/bins_size[rule])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a549546",
   "metadata": {},
   "source": [
    "### 2.1. - Distance at closest approach distribution analysis (`miss_distance`)\n",
    "\n",
    "The starting point for the process to generate additional synthetic data to be used to improve the training of the DL model is the analysis of the `miss_distance`. By analysing the distribution of the miss distance it is possible increase the frequency of conjunctions throughout time while ensuring reliability of data. To do so, we first have a look to the data distribution using an histogram plot as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data to plot\n",
    "data = df_lastCDM['miss_distance'].to_numpy()\n",
    "\n",
    "# Plot histogram of miss_distance\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.hist(data, bins = nbins(data, 'fd'), edgecolor='white')\n",
    "plt.xlim(0, data.max())\n",
    "plt.ylabel(r'Nb. of events')\n",
    "plt.xlabel(r'Miss distance (m)')\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab53726",
   "metadata": {},
   "source": [
    "### 2.2. - Keplerian elements distribution analysis\n",
    "\n",
    "In this section the following orbital elements are analysed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232bd66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define orbital elements columns dictionary\n",
    "keColumns = {'_h_per':   'Perigee (km)',\n",
    "             '_j2k_sma': 'Semi-major axis (km)',\n",
    "             '_j2k_inc': 'Inclination (deg)',\n",
    "             '_j2k_ecc': 'Eccentricity', \n",
    "             }\n",
    "\n",
    "# Iterate over all orbital elements\n",
    "for eType, eName in keColumns.items():\n",
    "    \n",
    "    # Get orbital element data from both target and chaser objects to compute outliers boundaries\n",
    "    data = pd.concat([df_lastCDM['t' + eType], df_lastCDM['c' + eType]]).to_numpy()\n",
    "    \n",
    "    # Calculate number of outliers for the orbital element\n",
    "    std_lims = outliers_boundaries(data, 2.0)\n",
    "    outliers = (data<std_lims[0]) | (data>std_lims[1])\n",
    "\n",
    "    \n",
    "    # Create figure object\n",
    "    fig, axs = plt.subplots(nrows=2, figsize=(10, 5), squeeze=True, \n",
    "                            sharex=True, sharey=True)\n",
    "    \n",
    "    # Set plot title to indicate boundaries for standard data and outliers excluded from histogram.\n",
    "    axs[0].set_title(r'Std. data limits: [{:.3f}, {:.3f}] \\qquad Objects: {:d} \\qquad Outliers: {:d} ({:.2f}\\%)'\\\n",
    "                     .format(std_lims[0], std_lims[1], len(data),  \n",
    "                             outliers.sum(), outliers.sum()/len(df_lastCDM.index)*100), \n",
    "                     fontsize=10)\n",
    "    \n",
    "    # Get minimum and maximum from data\n",
    "    xlim = (std_lims[0], std_lims[1])\n",
    "    \n",
    "    # Plot charts in subplot for both objects\n",
    "    colors = ['tab:blue','tab:orange']\n",
    "    \n",
    "    # Calculate number of bins to plot histogram \n",
    "    bins = nbins(data[outliers==False], 'fd')\n",
    "    \n",
    "    for i, obj in enumerate(['target', 'chaser']):\n",
    "        \n",
    "        \n",
    "        # Plot histogram for the object and keplerian element.\n",
    "        axs[i].hist(df_lastCDM[obj[0] + eType], bins = bins, \n",
    "                    color=colors[i], edgecolor='white', range=xlim, \n",
    "                    label=r'' + obj.capitalize())\n",
    "        \n",
    "        # Get the limits of Y-axis to reformat plot.\n",
    "        ylim = axs[i].get_ylim()\n",
    "        \n",
    "        yscale = 10**(math.floor(math.log(ylim[1], 10)))\n",
    "        \n",
    "        ylim = (ylim[0], math.ceil(ylim[1]/yscale)*yscale)\n",
    "\n",
    "        axs[i].grid(True, linestyle='--')\n",
    "        axs[i].set_yticks(np.linspace(ylim[0],ylim[1],5))\n",
    "        axs[i].set_xlim(xlim)\n",
    "        axs[i].set_ylim(ylim)\n",
    "        \n",
    "        axs[i].set_ylabel(r'Nb. of objects')\n",
    "        \n",
    "        # Plot label to identify which object the chart belongs to\n",
    "        axs[i].text(0.95, 0.95, r'' + obj.capitalize(), size=10, ha='center', va='top', \n",
    "                    c=colors[i], transform=axs[i].transAxes, \n",
    "                    bbox=dict(facecolor='white', alpha=0.75, edgecolor='white', pad=-1))\n",
    "\n",
    "    \n",
    "    axs[1].set_xlabel(r'' + eName)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81051538",
   "metadata": {},
   "source": [
    "# 3. - Probability Density Estimation for synthetic data generation\n",
    "\n",
    "[Introduction to Probability Density Estimation](https://machinelearningmastery.com/probability-density-estimation/)\n",
    "\n",
    "This analysis includes the two different ways to estimate the probability density associated to a given data distribution:\n",
    "\n",
    " - **Parametric**: the statistical distribution of the real data is described by an analytical and well-known statistical probability distribution by estimating the parameters implicit in the function.\n",
    " \n",
    " - **Non-parametric**: alternative method when the data distribution cannot be appropriately described analytically by a common probability distribution or cannot be easily made to fit the distribution (as it happens with multi-modal distributions). Among the multiple non-parametric methods available for Probability Density Estimation, the method used for estimating the probability density function of a continuous random variable in this analysis is the so-called Kernel Density Estimation (KDE).\n",
    " \n",
    "The probability density function with wich all relevant continuous variables for which new data shall be generated, are fitted using both parametric and non-parametric methods and compared between each other to identify the one with the lowest Error Sum of Squares (SSE). For the parametric approach, a function is defined to find the statistical distribution that best fits the real data distribution of all existing continuous distributions available in [SciPy.org](https://docs.scipy.org/doc/scipy/reference/stats.html). In the non-parametric approach the `KernelDensity` estimator is used from [Scikit-learn.org](https://scikit-learn.org/stable/modules/density.html) to fit a function to the continuous variable.\n",
    "\n",
    "### 3.1. - Parametric Probability Density estimation\n",
    "\n",
    "The process to fit a parametric probability density estimation follows a series of steps for which it is convenient to develop segregated functions. For this purpose, the following functions are defined:\n",
    " - `fit_distribution`: Fits a statistical continuous distribution to the input data. It returns the distribution, its parameters adjusted, and the resulting SSE.\n",
    " - `make_pdf`: It creates an array to represent the probability density function using a given fitted distribution and its parameters.\n",
    " - `plot_dist`: Plots the probability density function and the data distribution histogram on a single chart for a visual interpretation of the probability density estimation. It supports the reader/data analyst to check appropriate fitting; some distributions may return low SSE but does not appropriately describe de distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0206ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to fit statistical distribution to the data.\n",
    "def fit_distribution(distribution, data, bins):\n",
    "    \n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "    \n",
    "    # Try to fit the distribution\n",
    "    tic = time.process_time()\n",
    "    try:\n",
    "        # Ignore warnings from data that can\"t be fit\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "            # Fit dist to data\n",
    "            fitting_output = distribution.fit(data)\n",
    "\n",
    "            # Separate parts of parameters\n",
    "            parameters = {'loc':        fitting_output[-2],\n",
    "                          'scale':      fitting_output[-1],\n",
    "                          'arg':        fitting_output[:-2],\n",
    "                          'arg_names':  distribution.shapes.split(\", \") if distribution.shapes!=None else []}\n",
    "    \n",
    "            # Calculate fitted PDF and error with fit in distribution\n",
    "            pdf = distribution.pdf(x, loc=parameters['loc'], scale=parameters['scale'], *parameters['arg'])\n",
    "            sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "    except Exception:\n",
    "        \n",
    "        parameters, sse = None, np.inf\n",
    "        pass\n",
    "    \n",
    "    toc = time.process_time()\n",
    "    \n",
    "    # identify if this distribution is better\n",
    "    output = {'distribution': distribution, 'name': distribution.name, \n",
    "              'parameters': parameters, 'sse': sse}\n",
    "    \n",
    "    output['performance'] = {'processing_time': toc-tic, 'bins':bins}\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae72315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate distributions's Probability Distribution Function \n",
    "def make_pdf(distribution, parameters, size=10000):\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg     = parameters['arg']\n",
    "    loc     = parameters['loc']\n",
    "    scale   = parameters['scale']\n",
    "\n",
    "    # Get same start and end points of distribution\n",
    "    start = distribution.ppf(1e-4, *arg, loc=loc, scale=scale) \\\n",
    "        if arg else distribution.ppf(1e-4, loc=loc, scale=scale)\n",
    "    end = distribution.ppf(1-1e-4, *arg, loc=loc, scale=scale) \\\n",
    "        if arg else distribution.ppf(1-1e-4, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function to plot Probability Density Function with the histogram on the same chart\n",
    "def plot_distribution(data, distribution, parameters, bins):\n",
    "    \n",
    "    # Make Probability Density Function with distribution parameters \n",
    "    pdf = make_pdf(distribution, parameters)\n",
    "            \n",
    "    # Create string with all parameters to print into the plot's title\n",
    "    title = r'Stats model: \\texttt{' + distribution.name + r'}\\small' + '\\n' + \\\n",
    "                \"\\quad \".join([\"{} = {:0.3f}\".format(k,v) \\\n",
    "                for k,v in zip(parameters['arg_names'] + [\"loc\", \"scale\"], \n",
    "                               parameters['arg'] + parameters['loc'] + parameters['scale'])])\n",
    "    \n",
    "\n",
    "    # Display plot\n",
    "    plt.figure(figsize=(7,3))\n",
    "    \n",
    "    ax = plt.axes()\n",
    "    t = ax.text(0.5, 0.975, title, size=10, ha='center', va='top', \\\n",
    "                c='black', transform=ax.transAxes, \\\n",
    "                bbox=dict(facecolor='white', alpha=0.75, edgecolor='white', pad=-1))\n",
    "    \n",
    "    plt.plot(pdf.index.to_numpy(), pdf.values, lw=1.5, color = \"orange\", label=\"PDF\")\n",
    "    n, xbins, patches = plt.hist(data, bins=bins, density=True, histtype='bar', \n",
    "                                 color=\"dimgrey\", edgecolor = \"white\", label=\"Data\")\n",
    "    \n",
    "    bins_width = (xbins[1]-xbins[0])\n",
    "    plt.xlim(xbins[0] - bins_width, xbins[-1] + bins_width)\n",
    "    plt.ylim(0, np.max(n)*1.5)\n",
    "    \n",
    "    plt.title(r'Actual data distribution vs. fitted PDF', fontsize=12)\n",
    "    # plt.xlabel(data.name)\n",
    "    plt.ylabel(r\"Probability density\")\n",
    "    plt.grid(True, linestyle=\"dashed\", alpha=0.5)\n",
    "    plt.legend(loc=\"best\", fontsize=10)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9160b",
   "metadata": {},
   "source": [
    "#### Finding the best statistical model to recreate real data distributions.\n",
    "\n",
    "As described previously, for the parametric approach the function `find_best_distribution` is defined to find the statistical distribution that best fits the real data distribution of all existing continuous distributions available in [SciPy.org](https://docs.scipy.org/doc/scipy/reference/stats.html). To do so, the algorithm retrieves all existing continuous distributions, tries to fit it to the real data, computes the SSE and compare to the best fit found so far. It returns the best statistical distribution which corresponds to the one with the lowest SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d36aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get the list of continuous distributions from SciPy.org website\n",
    "def get_scipy_distributions():\n",
    "    try:\n",
    "        # Get Continuous distributions table from SciPy website.\n",
    "        url = 'https://docs.scipy.org/doc/scipy/reference/stats.html'\n",
    "        tbody = pd.read_html(requests.get(url).content)[1]\n",
    "        \n",
    "        # Create pandas dataframe and save CSV local file to be able to use the notebook offline.\n",
    "        df_distributions = pd.DataFrame(data = tbody[0].to_list(), columns=['scipy_distributions'])\n",
    "        df_distributions.to_csv(os.path.join(cwd,'temp_scipy-distributions.csv'), sep=',')\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Could not read SciPy.org website. Importing data from local file...\")\n",
    "        # Import training dataset\n",
    "        df_distributions = pd.read_csv(os.path.join(cwd,'data','notebook-outputs','temp_scipy-distributions.csv'), \n",
    "                            sep=',', header=0, index_col=None, skipinitialspace=False)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    # Evaluate list of objects in str format to convert it into a Python list\n",
    "    distributions_list = []\n",
    "    \n",
    "    # Iterate through all the continous distributions on the website and evaluate it\n",
    "    # to discard those that are not compatible with the library version installed.\n",
    "    for distribution_i in df_distributions['scipy_distributions'].to_list():\n",
    "        try:\n",
    "            distributions_list.append(eval('st.' + distribution_i))\n",
    "            print(distribution_i)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return distributions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b090d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to find best fitted distribution (according to lowest SSE)\n",
    "def find_best_distribution(data, bins, exclusion_list=[]):\n",
    "    \n",
    "    # Evaluate list of objects in str format to convert it into a Python list\n",
    "    distributions_list = get_scipy_distributions()\n",
    "    \n",
    "    # Initialize best holder using the norm distribution\n",
    "    best_holder = fit_distribution(st.norm, data, bins)\n",
    "    \n",
    "    fitting_results = []\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for i, distribution_i in enumerate(distributions_list):\n",
    "        \n",
    "        # If statistical distribution is in the exclusion list skip fitting process.\n",
    "        if distribution_i.name in exclusion_list: continue\n",
    "        \n",
    "        # Fit stdist to real data\n",
    "        fitting_output = fit_distribution(distribution_i, data, bins)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print('Progress %5.1f%% (%3d/%3d)  Best: %10s (SSE: %1.3e)  Distribution: %10s (SSE: %1.3e) \\t' %\n",
    "              ((i+1)/len(distributions_list)*100, i, len(distributions_list)-1, \n",
    "               best_holder['name'], best_holder['sse'], \n",
    "               fitting_output['name'], fitting_output['sse']), end='\\r')\n",
    "        \n",
    "        # If it improves the current best distribution, reassign best distribution\n",
    "        if best_holder['sse'] > fitting_output['sse'] > 0: best_holder = fitting_output\n",
    "        \n",
    "        fitting_results.append([fitting_output['name'], fitting_output['parameters'], \n",
    "                                fitting_output['sse'], fitting_output['performance']])\n",
    "        \n",
    "    # Sort values of dataframe by sse ascending and and re-index.\n",
    "    ranking = pd.DataFrame(data=fitting_results, columns=['distribution', 'parameters', 'sse', 'performance'])\n",
    "    ranking.sort_values(by=['sse'], axis='index', ascending=[True], \n",
    "                              inplace=True, ignore_index=True)\n",
    "    \n",
    "    # Clear output to print final results\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Print final results\n",
    "    # print(\"\\nBest distribution: \\n %s\" % (json.dumps(best_holder.pop('distribution'), sort_keys=True, indent=4)))\n",
    "    \n",
    "    return best_holder, ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25833232",
   "metadata": {},
   "source": [
    "#### Evaluation of  `miss_distance` using the parametric approach\n",
    "\n",
    "Computing `find_best_distribution` function over all relevant continuous variables the results are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e32bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set column name to study and remove outliers.\n",
    "data = df_lastCDM['miss_distance']\n",
    "\n",
    "# Find distribution that best fits the data\n",
    "# distribution = fit_distribution(st.kappa4, data, bins = nbins(data, 'fd'))\n",
    "distribution, ranking = find_best_distribution(data, bins = nbins(data, 'fd'), \n",
    "                                               exclusion_list = ['studentized_range', \n",
    "                                                                 'levy_l_gen', \n",
    "                                                                 'levy_stable'])\n",
    "    \n",
    "# Print plot including histogram and distribution fitted\n",
    "plot_distribution(data, distribution['distribution'], distribution['parameters'], bins = nbins(data, 'fd'))\n",
    "# ranking.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003189bf",
   "metadata": {},
   "source": [
    "The continuous statistical model that best describes the `miss_distance` data distribution is `kappa4`. In order to compare side by side real data versus the synthetic data a histogram with the same number of events is generated using the trained model below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6175d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get best results parameters from the ranking dataset on miss_distance variable\n",
    "best_stmodel = ranking.loc[0].to_dict()\n",
    "# print(json.dumps(best_stmodel, sort_keys=True, indent=4))\n",
    "\n",
    "# Get the distribution object from the best result\n",
    "distribution = getattr(st, best_stmodel['distribution'])\n",
    "\n",
    "# Generate random distribution using rvs function.\n",
    "synthetic_data = distribution.rvs(*best_stmodel['parameters']['arg'],\n",
    "                                 loc   = best_stmodel['parameters']['loc'],\n",
    "                                 scale = best_stmodel['parameters']['scale'], \n",
    "                                 size=500000, random_state=1)\n",
    "\n",
    "# Plot actual vs synthetic data for comparison\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.hist([data, synthetic_data], density=True,\n",
    "         bins = nbins(data, 'fd'), \n",
    "         label=['Actual', 'Synthetic'], \n",
    "         color=['dimgrey','lightskyblue'])\n",
    "plt.ylabel(r'Probability density of ocurrence', fontsize=10)\n",
    "plt.xlabel(r'Miss distance (m)', fontsize=10)\n",
    "plt.title(r'Actual vs synthetic data distribution'+ '\\n' +'\\small (Stats model ' + distribution.name + ')')\n",
    "plt.grid(True, linestyle='dashed', alpha=0.5)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69159f",
   "metadata": {},
   "source": [
    "### 3.2. - Non-Parametric Probability Density Estimation using KDE\n",
    "\n",
    "The keplerian elements that describle the orbits for both targets and chasers objects shows multimodal distributions that cannot be described by continuous stats models. As a consequence, the Non-parametric Probability Density Estimation (PDE) with KDE shall be used in order to fit a model that allows for synthetic data generation with a realistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95cacd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "# Run through both type of objects\n",
    "for keColumn, keLabel in keColumns.items():\n",
    "\n",
    "    # Get the data upon which the clustering shall be done\n",
    "    data = np.array(df_lastCDM['t' + keColumn]).reshape(-1, 1)\n",
    "\n",
    "    # Get number of bins used in a global histogram for better readability\n",
    "    bins = nbins(data, 'fd')\n",
    "\n",
    "    # Use Silhouette method to compute optimal number of clusters\n",
    "\n",
    "    potential_clusters = np.arange(2, 11,1)\n",
    "    silhouette_avg = []\n",
    "    for n_clusters in potential_clusters:\n",
    "\n",
    "        # Initialise the clusterer object\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n",
    "\n",
    "        # Calculate silhouette score and append it to the array\n",
    "        silhouette_avg.append(silhouette_score(data, clusterer.labels_))\n",
    "\n",
    "    # Get maximum Silhouette score that indicate optimal number of clusters\n",
    "    max_silhouette_score = np.max(silhouette_avg)\n",
    "    n_clusters = np.min(potential_clusters[silhouette_avg==max_silhouette_score])\n",
    "\n",
    "    # Use KMeans algorithm to identify all different clusters in 1D\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=0).fit(data)\n",
    "    \n",
    "    # Re-label clusters to be sorted by orbit region from lower to upper\n",
    "    # in order to visually identify cluster labels from plots\n",
    "    clusters_centers = clusterer.cluster_centers_.flatten()\n",
    "    clusters_labels = clusterer.predict(clusters_centers.reshape(-1,1))\n",
    "    \n",
    "    labels  = clusterer.labels_.flatten()\n",
    "\n",
    "    custom_labels = np.zeros_like(labels)\n",
    "    for c, center in enumerate(np.sort(clusters_centers)):\n",
    "        label = clusters_labels[clusters_centers==center]\n",
    "        custom_labels[labels == label] = c\n",
    "\n",
    "\n",
    "    # Create the figure object\n",
    "    fig = plt.figure(figsize=(10,2.5))\n",
    "\n",
    "    # Create grid for different subplots\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=1, width_ratios=[2, 3], wspace=0.25)\n",
    "\n",
    "    # Plot Silhouette chart\n",
    "    ax0 = fig.add_subplot(spec[0])\n",
    "    ax0.plot(potential_clusters, silhouette_avg,'kx--')\n",
    "    ax0.plot(n_clusters, max_silhouette_score, 'ro')\n",
    "    ax0.set_ylabel(r'Silhouette score')\n",
    "    ax0.set_xlabel(r'Nb. of clusters')\n",
    "    ax0.set_ylim(0.5,1)\n",
    "    ax0.set_xticks(potential_clusters)\n",
    "\n",
    "    # Plot histogram grouped by clusters\n",
    "    ax1 = fig.add_subplot(spec[1])\n",
    "    N, bins, patches = ax1.hist(data, bins = bins, edgecolor = 'white')\n",
    "    ax1.set_ylabel(r'Nb. of events')\n",
    "    ax1.set_xlabel(r'' + keLabel)\n",
    "    \n",
    "    bins_per_cluster = dict(zip(clusters_labels, [0]*len(clusters_labels)))\n",
    "    print(bins_per_cluster)\n",
    "\n",
    "    for i in range(len(patches)):\n",
    "\n",
    "        # Get the cluster to which the bin belongs to\n",
    "        bin_cluster = custom_labels[(data<=bins[i+1]).flatten() & (data>bins[i]).flatten()]\n",
    "        \n",
    "        if len(bin_cluster)>0: \n",
    "            bins_per_cluster[bin_cluster[0]] += 1\n",
    "            # Change histogram bar color if it belongs to a cluster\n",
    "            patches[i].set_facecolor(colors[np.max(bin_cluster)])\n",
    "\n",
    "    ax0.grid(True, linestyle=\"dashed\", alpha=0.5)\n",
    "    ax1.grid(True, linestyle=\"dashed\", alpha=0.5)\n",
    "    \n",
    "    fig.suptitle(r'Data distribution cluster analysis for \\texttt{t' + keColumn + '}' + '\\n'\n",
    "                 '\\small (Optimum number of clusters = ' + str(n_clusters) + ')', \n",
    "                 fontsize=12, y=1.05)\n",
    "    \n",
    "    plt.show()\n",
    "    print(bins_per_cluster)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeb1fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the data upon which the clustering shall be done\n",
    "data = np.array(df_lastCDM['t_h_per']).reshape(-1, 1)\n",
    "\n",
    "# Clusters 1, 3 can be fitted with norm (cluster 4 could potentially be included) \n",
    "# Cluster 2 can be fitted with mielke\n",
    "n_cluster = 4\n",
    "sub_data = data[custom_labels==n_cluster]\n",
    "bins = bins_per_cluster[n_cluster]\n",
    "\n",
    "# Find distribution that best fits the data\n",
    "# distribution = fit_distribution(st.kappa4, data, bins = nbins(data, 'fd'))\n",
    "distribution, ranking = find_best_distribution(sub_data, bins = bins, \n",
    "                                               exclusion_list = ['studentized_range', \n",
    "                                                                 'levy_l_gen', \n",
    "                                                                 'levy_stable'])\n",
    "    \n",
    "# Print plot including histogram and distribution fitted\n",
    "plot_distribution(sub_data, distribution['distribution'], distribution['parameters'], bins=bins)\n",
    "\n",
    "\n",
    "distribution = fit_distribution(st.norm, sub_data, bins = bins)\n",
    "plot_distribution(sub_data, distribution['distribution'], distribution['parameters'], bins=bins)    \n",
    "\n",
    "\n",
    "ranking.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fc73a",
   "metadata": {},
   "source": [
    "#### Probability Density Estimation using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d81e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize array with all available kernels in KernelDensity\n",
    "kernels = [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n",
    "\n",
    "# Initialize array with different values for the bandwidth\n",
    "bandwidths = np.arange(2,20,2)\n",
    "\n",
    "\n",
    "# Get the data upon which the clustering shall be done\n",
    "data = np.array(df_lastCDM['t_h_per']).reshape(-1, 1)\n",
    "\n",
    "# Clusters 1, 3 can be fitted with norm \n",
    "# Cluster 2 can be fitted with mielke\n",
    "n_cluster = 2\n",
    "sub_data = data[custom_labels==n_cluster]\n",
    "bins = bins_per_cluster[n_cluster]\n",
    "\n",
    "# Get probability density from real data\n",
    "dens_data, bin_edges = np.histogram(sub_data, bins=bins, density=True)\n",
    "\n",
    "X = (bin_edges + np.roll(bin_edges, -1))[:-1] / 2.0\n",
    "\n",
    "\n",
    "# Create plot object to compare all kernels as a function of the bandwidth\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "# Iterate through all kernels\n",
    "for kernel in kernels:\n",
    "    \n",
    "    # Initialize array to store SSE\n",
    "    sse = np.zeros_like(bandwidths, dtype=float)\n",
    "\n",
    "    # Iterate through bandwidths\n",
    "    for i, bandwidth in enumerate(bandwidths):\n",
    "\n",
    "\n",
    "        # Fit the Kernel Density Estimator\n",
    "        kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(sub_data)\n",
    "\n",
    "        # Compute the exponential of the log-likelihood of each sample under the model.\n",
    "        dens_estimated = np.exp(kde.score_samples(X.reshape(-1,1)))\n",
    "\n",
    "        # Compute Sum of Squared Errors (SSE) for the density estimation\n",
    "        sse[i] = np.sum((dens_estimated - dens_data)**2)\n",
    "\n",
    "    print(\"Min. SSE: %.4e\\t Bandwidth: %.3f\\t Kernel: %s\" % (np.min(sse), bandwidths[sse==np.min(sse)][0], kernel))\n",
    "\n",
    "    plt.plot(bandwidths, sse, label=r'' + kernel)\n",
    "\n",
    "plt.title(r'Comparison between Kernels given a bandwidth')\n",
    "\n",
    "plt.ylabel(r'Sum of Squared Errors (SSE)')\n",
    "plt.xlabel(r'Bandwidth')\n",
    "\n",
    "# Define X-axis ticks and limits\n",
    "plt.xticks(np.arange(0.0,bandwidths.max(),0.5))\n",
    "plt.xlim(0.0, bandwidths.max())\n",
    "\n",
    "# Define Y-axis ticks and limits\n",
    "#plt.yticks(np.linspace(0.0, sse.max(),10))\n",
    "#plt.ylim(0.0, sse.max())\n",
    "\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for kernel in kernels:\n",
    "\n",
    "    # Create plot object to compare all kernels as a function of the bandwidth\n",
    "    plt.figure(figsize=(6,3))\n",
    "\n",
    "    \n",
    "    plt.hist(sub_data, label=r'Real data', density=True)\n",
    "    for bandwidth in bandwidths:\n",
    "        \n",
    "    \n",
    "        # Fit the Kernel Density Estimator\n",
    "        kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(sub_data)\n",
    "\n",
    "        # Compute the exponential of the log-likelihood of each sample under the model.\n",
    "        X_estimated = np.linspace(np.min(X), np.max(X), 100)\n",
    "        dens_estimated = np.exp(kde.score_samples(X_estimated.reshape(-1,1)))\n",
    "        \n",
    "        plt.plot(X_estimated, dens_estimated, label=r'' +'{:.2f}'.format(bandwidth))\n",
    "    plt.title(r'' + kernel)\n",
    "    plt.ylabel(r'Probability Density')\n",
    "    plt.xlabel(r'Bandwidth')\n",
    "\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807dab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_lastCDM['miss_distance'].to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(KernelDensity(kernel='exponential'),\n",
    "                    {'bandwidth': np.linspace(120, 130, 20)},\n",
    "                    cv=5) # 20-fold cross-validation\n",
    "grid.fit(data)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c84261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get both object's data\n",
    "data = sub_data\n",
    "\n",
    "# Create kernel to estimate probability over the population\n",
    "kde = st.gaussian_kde(data, bw_method='scott')\n",
    "    \n",
    "# Get minimum and maximum from data\n",
    "x = np.linspace(data.min(), data.max(), 100)\n",
    "\n",
    "# Plot the probability density distribution estimated function\n",
    "plt.figure(figsize=(6,3), layout='tight')\n",
    "plt.plot(x, kde(x))\n",
    "plt.title(r'Probability density estimate')\n",
    "plt.xlabel(r'Miss distance (km)')\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through both type of objects\n",
    "for eType, eName in keColumns.items():\n",
    "    \n",
    "    # Get both object's data\n",
    "    data = df_lastCDM[['t' + eType,'c' + eType]]\n",
    "    \n",
    "    # Get minimum and maximum from data\n",
    "    xlim = (data.min().min(), data.max().max())\n",
    "    \n",
    "    x = np.linspace(xlim[0], xlim[1], 100)\n",
    "    \n",
    "    plt.figure(figsize=(6,3), layout='tight')\n",
    "    \n",
    "    for oType, oPreffix in objects.items():\n",
    "    \n",
    "        # Create kernel to estimate probability over the population\n",
    "        kde = st.gaussian_kde(data[oPreffix + eType], bw_method='scott')\n",
    "        \n",
    "        plt.plot(x, kde(x), label=r'' + oType.capitalize())\n",
    "        \n",
    "    # print('KDE Bandwith method = %.4f' % kde.n**(-1./(kde.d+4)))\n",
    "    plt.title(r'Probability density estimate')\n",
    "    plt.xlabel(eName)\n",
    "    plt.grid(True, linestyle='--')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
